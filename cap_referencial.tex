\chapter{Referencial Teórico}

Este capítulo apresenta os fundamentos conceituais e técnicos que
sustentam o desenvolvimento de um gerador de modelos de simulação
orientado por mineração de processos. São
abordados desde os conceitos fundamentais de mineração de processos
até a integração com simulação de eventos discretos e indicadores de
desempenho operacional aplicados a ambientes hospitalares.

\section{Mineração de Processos}

A mineração de processos  é uma disciplina
que une conceitos de mineração de dados e de
gerenciamento de processos de negócio com o objetivo de extrair conhecimento útil a
partir de registros de eventos provenientes de
sistemas de informação. Segundo van der Aalst
\cite{aalst2016process}, a mineração de processos tem como propósito
descobrir, monitorar e aprimorar processos reais com base nos dados
efetivamente registrados, constituindo uma ponte entre a análise
orientada a dados e a modelagem formal de processos.

Com a crescente digitalização das operações empresariais e o avanço
de sistemas como ERPs, CRMs e sistemas de controle de manufatura,
passou a ser possível registrar detalhadamente cada etapa executada
em um processo. Esses registros, conhecidos como \textit{event logs},
formam a base para a aplicação de técnicas de mineração de processos.
Cada evento registrado representa a execução de uma atividade
pertencente a um caso e contém atributos como o nome
da atividade, o identificador do caso, o responsável
(\textit{resource}) e o carimbo de tempo (\textit{timestamp}). A
estrutura desses logs permite a reconstituição da sequência de
atividades e o estudo do comportamento do processo real
\cite{maruster2009redesigning}.

A mineração de processos é composta por três grandes categorias de
técnicas \cite{aalst2016process}:
\begin{enumerate}
	\item \textbf{Descoberta de processos (\textit{Process Discovery})}: tem como objetivo gerar automaticamente um modelo de processo a partir de um log de eventos, sem conhecimento prévio do fluxo. O modelo resultante pode ser representado em diferentes notações, como Redes de Petri, BPMN (\textit{Business Process Model and Notation}) ou Árvores de Processo (\textit{Process Trees}).
	\item \textbf{Verificação de conformidade (\textit{Conformance Checking})}: consiste em comparar um modelo de processo pré-existente com um log de eventos real, avaliando a aderência entre o comportamento observado e o comportamento esperado. Essa comparação fornece métricas como \textit{fitness} e \textit{precision}.
	\item \textbf{Aprimoramento de modelos (\textit{Enhancement})}: busca enriquecer modelos existentes com informações adicionais provenientes dos logs, como tempos médios de execução, gargalos, desvios e uso de recursos, permitindo a análise de desempenho e a detecção de oportunidades de otimização.
\end{enumerate}

O ciclo de mineração de processos, descrito no \textit{Process Mining
	Manifesto} da IEEE Task Force on Process Mining
\cite{aalst2016process}, enfatiza que a aplicação prática da técnica
depende da disponibilidade e da qualidade dos logs de eventos. Logs
incompletos, inconsistentes ou mal formatados comprometem a acurácia
dos modelos descobertos. Nesse sentido, Kherbouche et al.
\cite{betterlogs2020} destacam a importância da avaliação da
qualidade dos logs, propondo métricas de \textit{completude},
\textit{consistência} e \textit{complexidade} antes da aplicação das
técnicas de mineração.

Em termos de arquitetura, um sistema de mineração de processos segue
um fluxo básico: coleta de dados, pré-processamento, mineração
propriamente dita e análise dos resultados. Durante a coleta, os logs
podem ser extraídos de sistemas como SAP, Oracle, ou de bancos de
dados customizados. O pré-processamento envolve a limpeza e
padronização dos dados, incluindo a identificação correta de casos e
atividades. Em seguida, algoritmos como \textit{Alpha Miner},
\textit{Heuristic Miner} e \textit{Inductive Miner} são aplicados
para a descoberta do modelo de processo
\cite{leemans2013discovering}.

Os resultados podem ser apresentados em notações formais, como Redes
de Petri, ou em linguagens mais visuais, como BPMN. O uso de
ferramentas especializadas, como o \textit{ProM Framework} e a
biblioteca Python PM4Py, facilita essa análise e integração com
outros métodos, como a simulação de eventos discretos
\cite{ferronato2021pm2sim}.

O padrão \textit{eXtensible Event Stream} (XES), definido pela IEEE
\cite{ieeexes2010}, estabelece a estrutura de logs de eventos para
garantir interoperabilidade entre ferramentas e consistência na troca
de dados. Cada log em XES é composto por uma coleção de
\textit{traces} (casos), e cada \textit{trace} contém uma sequência
ordenada de \textit{events}. Essa padronização é essencial para o
sucesso de frameworks modernos de mineração de processos e simulação
integrada.

Portanto, a mineração de processos se consolida como uma abordagem
essencial para compreender, auditar e melhorar processos
organizacionais em ambientes baseados em dados. Quando combinada com
simulação e análise estatística, ela se torna uma ferramenta poderosa
para suporte à decisão, otimização operacional e melhoria contínua de
processos complexos, como os hospitalares e logísticos.

\section{Inductive Miner}

Entre os diversos algoritmos de descoberta de processos disponíveis,
o \textit{Inductive Miner} (IM), proposto por Leemans, Fahland e van
der Aalst \cite{leemans2013discovering}, é um dos mais relevantes e
amplamente utilizados na literatura e em ferramentas modernas de
mineração, como o \textit{ProM} e o \texttt{PM4Py}.

Diferentemente de abordagens anteriores, como o \textit{Alpha Miner}
e o \textit{Heuristic Miner}, o Inductive Miner foi projetado para
garantir propriedades formais no modelo resultante, como a
\textit{soundness} (correção comportamental) e a estrutura
hierárquica em blocos. Essas propriedades
asseguram que o modelo possa ser executado sem estados mortos ou
impasses, além de facilitar sua conversão em linguagens formais como
Redes de Petri e Árvores de Processo.

O princípio fundamental do algoritmo baseia-se na decomposição
recursiva do log de eventos. O IM analisa as relações de precedência
e causalidade entre atividades e divide o log em sublogs coerentes,
representando fragmentos independentes do processo. Cada sublog é
então minerado de forma isolada, e os resultados são combinados em
uma estrutura hierárquica que reflete a composição lógica das
atividades.

Uma das vantagens práticas do Inductive Miner é sua compatibilidade
direta com representações de simulação. Ao produzir modelos
formalmente corretos e livres de inconsistências estruturais, o IM
facilita a transformação automática em modelos de simulação baseados
em Redes de Petri, como destacado por Ferronato
\cite{ferronato2021pm2sim}. Essa característica é essencial para o
desenvolvimento de sistemas como o \textit{PM2Sim}, que automatiza a
criação de modelos de simulação de eventos discretos.

\section{Redes de Petri}

As Redes de Petri constituem um formalismo matemático e gráfico
amplamente utilizado para modelar, analisar e simular sistemas de
eventos discretos. Propostas originalmente por Carl Adam Petri na
década de 1960 e formalizadas por Peterson \cite{peterson1981petri},
essas redes oferecem uma base rigorosa para a representação de
processos dinâmicos caracterizados por concorrência, sincronização,
conflito e causalidade — propriedades típicas de sistemas produtivos,
logísticos e hospitalares.

Uma Rede de Petri é definida formalmente como uma tupla $N = (P, T,
	F)$, onde:
\begin{itemize}
	\item $P$ representa o conjunto de lugares (\textit{places});
	\item $T$ representa o conjunto de transições (\textit{transitions});
	\item $F \subseteq (P \times T) \cup (T \times P)$ é o conjunto de arcos que conectam lugares e transições.
\end{itemize}

Os lugares simbolizam condições ou estados do sistema, enquanto as
transições representam eventos ou atividades que modificam esses
estados. A dinâmica do sistema é descrita por meio da movimentação de
fichas (\textit{tokens}) entre os lugares. O conjunto de tokens em um
dado instante define a marcação atual da rede (\textit{marking}),
representando o estado do processo. Quando todas as condições de
disparo de uma transição são satisfeitas, ela se torna habilitada e
pode ser executada, consumindo e produzindo tokens conforme o fluxo
definido em $F$. Essa semântica de disparo possibilita a modelagem de
sistemas paralelos e assíncronos de forma intuitiva e precisa.

Segundo Peterson \cite{peterson1981petri}, uma das principais
vantagens das Redes de Petri é a possibilidade de realizar análises
formais de propriedades do sistema modelado, como:
\begin{itemize}
	\item \textbf{Alcançabilidade} (\textit{Reachability}): determina quais estados podem ser atingidos a partir da marcação inicial.
	\item \textbf{Viveza} (\textit{Liveness}): garante que nenhuma transição se torne permanentemente inativa, evitando estados mortos.
	\item \textbf{Conservação} (\textit{Boundedness}): assegura que o número de tokens em cada lugar permanece finito, prevenindo explosões de estados.
	\item \textbf{Deadlock-freedom}: certifica que o sistema não entra em impasse completo.
\end{itemize}

Essas propriedades são fundamentais para a verificação de correção
comportamental (\textit{soundness}) em modelos de processos
descobertos via mineração, assegurando que o modelo é executável e
que todo caso pode ser concluído corretamente
\cite{aalst2016process}.

No contexto da mineração de processos, as Redes de Petri são
amplamente utilizadas como formalismo intermediário para representar
os modelos extraídos de logs de eventos. O \textit{Inductive Miner},
por exemplo, gera diretamente uma Rede de Petri \textit{sound} e
\textit{block-structured} \cite{leemans2013discovering}, permitindo
tanto a verificação de conformidade quanto a execução simulada do
processo. Essa característica é essencial para a integração entre
mineração e simulação de eventos discretos, pois possibilita a
tradução direta de modelos minerados em estruturas simuláveis.

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.8\textwidth]{figuras/fig1.png}
	\caption{Exemplo de Rede de Petri representando um processo de negócio. Fonte: Autor (2025)}
	\label{fig:rede_petri_exemplo}
\end{figure}

Na Figura~\ref{fig:rede_petri_exemplo}, os elementos da Rede de Petri
são representados da seguinte forma: os círculos representam
\textit{lugares} (places), que indicam estados ou condições do
processo; os retângulos representam \textit{transições}
(transitions), que correspondem a atividades ou eventos que podem
ocorrer; os círculos com pontos pretos internos representam
\textit{tokens} (marcas), que indicam a presença de uma condição ou o
estado atual do processo; e os círculos com contorno destacado e
tokens internos representam lugares marcados, indicando estados
ativos no momento atual da execução.

Ferronato \cite{ferronato2021pm2sim} destaca que as Redes de Petri
desempenham um papel central na integração entre mineração e
simulação. Em seu framework \textit{PM2Sim}, as redes extraídas a
partir de logs de eventos são automaticamente convertidas em modelos
de simulação de eventos discretos implementados em Python. Essa
conversão permite avaliar métricas como tempo de ciclo, gargalos e
utilização de recursos, transformando os modelos minerados em
instrumentos de suporte à decisão operacional.

Em síntese, as Redes de Petri constituem uma linguagem formal robusta
e expressiva para representar o comportamento de sistemas reais. Sua
adoção no contexto da mineração de processos garante não apenas a
fidelidade comportamental dos modelos gerados, mas também sua
viabilidade para análises de desempenho e simulação, consolidando-se
como elo fundamental entre a teoria de processos e a prática da
modelagem computacional.

\section{Simulação de Eventos Discretos}

A simulação de eventos discretos é uma técnica de modelagem computacional amplamente utilizada
para representar sistemas dinâmicos em que o estado do sistema muda
apenas em pontos discretos no tempo. Cada mudança é provocada por um
evento, que ocorre em um instante específico e representa uma
transição de estado. Essa abordagem é amplamente empregada em áreas
como manufatura, logística, saúde e serviços, onde os processos são
compostos por atividades sequenciais, paralelas e dependentes de
recursos.

A DES permite reproduzir o comportamento de sistemas complexos sem a
necessidade de interferir em seu ambiente real, possibilitando
análises de desempenho, previsão de gargalos e avaliação de cenários
alternativos. Diferentemente da simulação contínua, que modela
fenômenos em tempo contínuo por meio de equações diferenciais, a
simulação discreta descreve processos orientados por eventos, sendo,
portanto, ideal para a representação de fluxos de trabalho e
processos empresariais.

Em termos formais, um modelo de simulação de eventos discretos é
composto por três elementos básicos:
\begin{itemize}
	\item \textbf{Entidades}: representam os objetos que transitam pelo sistema (por exemplo, pacientes, ordens de serviço ou produtos).
	\item \textbf{Recursos}: correspondem aos elementos que executam as atividades (como profissionais, máquinas ou salas cirúrgicas).
	\item \textbf{Eventos}: são as ocorrências que alteram o estado do sistema, como o início ou o término de uma atividade.
\end{itemize}

Esses componentes são orquestrados por um \textit{motor de simulação}
(\textit{simulation engine}) que mantém um relógio lógico e agenda os
próximos eventos a serem processados. Cada evento executado pode
gerar novos eventos futuros, modificando o estado do sistema e
permitindo a evolução temporal do modelo \cite{liu2015integrating}.

A integração entre simulação e mineração de processos vem sendo
explorada nos últimos anos. Liu \cite{liu2015integrating} demonstrou
que os modelos extraídos via \textit{process mining} podem ser
automaticamente convertidos em modelos de simulação de eventos
discretos, reduzindo o esforço de modelagem e aumentando a precisão
analítica. Essa integração é especialmente relevante em contextos
onde a dinâmica do sistema se altera frequentemente, exigindo
atualizações rápidas de modelos e previsões.

Nesse contexto, Ferronato e Scalabrin \cite{ferronato2021pm2sim}
desenvolveram o framework \textit{PM2Sim}, que automatiza a criação
de modelos de simulação a partir de logs de eventos reais. O sistema
identifica atividades, durações, tempos de espera e recursos
envolvidos, transformando essas informações em um modelo DES
implementado em Python com a biblioteca \texttt{SimPy}. Essa
biblioteca fornece uma estrutura orientada a processos, baseada em
geradores, que permite modelar entidades como processos que interagem
em um ambiente temporal discreto. A \texttt{SimPy} oferece ainda
suporte à criação de filas, controle de recursos, eventos simultâneos
e coleta de estatísticas de desempenho, tornando-a ideal para
aplicações em mineração de processos e análise operacional.

Além disso, o framework \textit{PM2Sim} utiliza distribuições
estatísticas ajustadas por meio de técnicas de \textit{fitting}
(adequação), com base em bibliotecas como \texttt{NumPy} e
\texttt{SciPy}, permitindo representar o comportamento dos tempos de
execução das atividades. O resultado é um modelo de simulação que
reflete o comportamento observado nos logs, possibilitando a análise
de métricas como tempo de ciclo, utilização de recursos e
identificação de gargalos.

De acordo com Wuennenberg et al. \cite{wuennenberg2023internal}, a
combinação de mineração de processos e simulação discreta é
particularmente útil em ambientes industriais e logísticos, onde a
qualidade dos dados e a variabilidade operacional representam
desafios significativos. Nesse sentido, a DES serve como ferramenta
de validação e experimentação para modelos minerados, permitindo
testar hipóteses e prever o impacto de mudanças estruturais antes de
sua implementação no ambiente real.

\section{Análise Estatística}

A etapa de análise estatística é fundamental para a construção de
modelos de simulação realistas e coerentes com os processos
observados. Após a descoberta do modelo de processo e a extração das
informações de desempenho a partir dos logs de eventos, é necessário
realizar o ajuste estatístico dos parâmetros temporais, como durações
de atividades, tempos de espera e intervalos entre eventos. Esses
parâmetros são essenciais para que o modelo de simulação reflita
adequadamente a variabilidade e o comportamento estocástico do
sistema.

Segundo van der Aalst \cite{aalst2016process}, a mineração de
processos deve ser vista como uma disciplina orientada por dados
(\textit{data-driven}), e o sucesso de suas aplicações depende da
correta interpretação das distribuições de tempo, frequência e
desempenho que emergem dos registros de eventos. O uso de técnicas
estatísticas complementa a fase de descoberta, permitindo transformar
modelos descritivos em modelos quantitativos capazes de realizar
simulações e análises preditivas.

Ferronato e Scalabrin \cite{ferronato2021pm2sim} destacam que o uso
de distribuições estatísticas é um dos pilares do framework
\textit{PM2Sim}, que automatiza a criação de modelos de simulação a
partir de logs reais. No sistema proposto, os tempos de execução das
atividades e os intervalos entre eventos são ajustados a partir de
funções de probabilidade clássicas, como Normal, Log-Normal e
Exponencial. Essa parametrização é obtida por meio do ajuste de
distribuições (\textit{distribution fitting}), realizado com base nos
dados históricos de cada atividade registrada no log. O processo
envolve a estimativa dos parâmetros das distribuições e a verificação
de aderência dos dados observados, assegurando a representatividade
do modelo.

Para realizar esses ajustes, bibliotecas estatísticas como
\texttt{NumPy} e \texttt{SciPy} são utilizadas no ambiente Python,
permitindo estimar as distribuições de probabilidade e calcular
medidas como média, variância, desvio padrão e coeficiente de
variação. Além disso, testes de aderência, como o de
Kolmogorov–Smirnov, são empregados para validar se os dados amostrais
seguem adequadamente a distribuição teórica selecionada. Esse
procedimento garante que os tempos de simulação não apenas reproduzam
as médias históricas, mas também capturem a variabilidade natural do
processo \cite{liu2015integrating}.

O resultado da análise estatística é incorporado diretamente aos
parâmetros do modelo de simulação, alimentando o mecanismo de eventos
discretos com tempos amostrados de distribuições probabilísticas.
Essa abordagem permite representar o comportamento dinâmico e
imprevisível dos processos reais, algo essencial em ambientes
sujeitos a variações operacionais, como hospitais e sistemas
logísticos. Conforme observado por Leemans et al.
\cite{leemans2013discovering}, a precisão dos tempos e a correta
modelagem da frequência das atividades impactam diretamente a
capacidade do modelo minerado em reproduzir o fluxo real dos eventos.

Assim, a análise estatística atua como uma ponte entre a descoberta
de processos e a simulação de eventos discretos, traduzindo dados
empíricos em parâmetros quantitativos para o modelo. Essa integração
garante que a simulação gerada preserve as características
estatísticas do processo original, permitindo a avaliação confiável
de métricas de desempenho e a experimentação de cenários alternativos
de operação.

\section{Métricas de Qualidade de Modelos}

A qualidade dos modelos descobertos por meio da mineração de
processos é um fator determinante para a confiabilidade das análises
subsequentes e, principalmente, para o uso desses modelos como base
para simulações. A avaliação sistemática da qualidade garante que o
modelo minerado não apenas represente corretamente o comportamento
histórico, mas também seja capaz de generalizar o comportamento
futuro e sustentar decisões operacionais baseadas em evidências.

De acordo com van der Aalst \cite{aalst2016process}, um modelo de
processo deve ser avaliado em múltiplas dimensões de qualidade, de
modo a equilibrar precisão, generalização e simplicidade. As
principais métricas utilizadas na literatura são: \textit{fitness},
\textit{precision}, \textit{generalization} e \textit{simplicity}.
Essas métricas, amplamente adotadas em ferramentas como o
\textit{ProM Framework} e a \texttt{PM4Py}, compõem a base dos
algoritmos de verificação de conformidade (\textit{conformance
	checking}), responsáveis por comparar o comportamento observado nos
logs de eventos com o comportamento previsto pelo modelo minerado.

\begin{itemize}
	\item \textbf{Fitness}: avalia o quanto o modelo reproduz o comportamento observado no log. Um modelo com alto \textit{fitness} consegue reproduzir todas as sequências válidas de atividades sem apresentar falhas de execução.
	\item \textbf{Precision}: mede o nível de restrição do modelo, penalizando comportamentos não observados nos dados. Modelos excessivamente permissivos tendem a apresentar alta flexibilidade, mas baixa precisão.
	\item \textbf{Generalization}: representa a capacidade do modelo de capturar variações plausíveis do processo, evitando o sobreajuste aos dados históricos.
	\item \textbf{Simplicity}: reflete o grau de complexidade estrutural do modelo; modelos excessivamente complexos, embora precisos, dificultam a interpretação e a manutenção \cite{manifesto2011}.
\end{itemize}

O \textit{Process Mining Manifesto} \cite{manifesto2011} destaca a
importância de equilibrar essas dimensões, pois a busca por um modelo
com \textit{fitness} perfeito pode levar à perda de generalização, e
vice-versa. A maturidade da área de mineração de processos se reflete
justamente na capacidade de lidar com esse compromisso entre precisão
e abstração. Esse equilíbrio é particularmente relevante quando os
modelos descobertos serão empregados em simulações, uma vez que
comportamentos não observados ou superajustados podem comprometer a
validade dos resultados simulados.

Liu \cite{liu2015integrating} ressalta que a integração entre
mineração e simulação exige não apenas um modelo logicamente
consistente, mas também estatisticamente representativo. A validação
entre logs reais e logs sintéticos simulados permite medir a
aderência entre ambos, utilizando métricas de distância e
similaridade, como a \textit{edit distance}. Essa abordagem garante
que a simulação não apenas reproduza o fluxo de atividades, mas
também mantenha coerência temporal e probabilística com o processo
original.

Kherbouche et al. \cite{betterlogs2020} complementam essa perspectiva
ao argumentar que a qualidade do modelo está diretamente relacionada
à qualidade do log de eventos. Logs incompletos, redundantes ou
ruidosos geram modelos com baixa precisão e menor confiabilidade. Por
isso, o controle da qualidade dos logs — avaliado por dimensões como
\textit{completude}, \textit{consistência}, \textit{acurácia} e
\textit{complexidade} — é pré-requisito para a obtenção de métricas
significativas de \textit{fitness} e \textit{precision}.

Portanto, as métricas de qualidade de modelos constituem um elo
crítico entre mineração e simulação. Elas fornecem os indicadores
necessários para validar a representatividade, robustez e
aplicabilidade do modelo descoberto. Um modelo de processo só é
efetivamente útil quando combina boa aderência aos dados históricos
com capacidade de generalização para novos cenários — característica
essencial para o uso em ambientes de tomada de decisão e otimização
operacional.

\section{Geração de Logs Sintéticos}

A geração de logs sintéticos é uma etapa estratégica na integração
entre mineração de processos e simulação de eventos discretos. Essa
técnica permite criar registros artificiais de eventos que preservam
as propriedades estruturais e estatísticas de logs reais,
viabilizando experimentos controlados, testes de desempenho e
validação de modelos sem a necessidade de utilizar dados sensíveis ou
restritos. Em termos práticos, os logs sintéticos servem como uma
representação simulada do comportamento observado, permitindo avaliar
a fidelidade dos modelos minerados e a confiabilidade das previsões
operacionais.

Segundo van der Aalst \cite{aalst2016process}, a utilização de logs
artificiais é essencial para verificar se o modelo de processo
descoberto é capaz de reproduzir o comportamento do sistema real.
Essa comparação é comumente feita por meio de técnicas de
\textit{conformance checking}, nas quais o log sintético gerado pelo
modelo é confrontado com o log original, medindo-se métricas como
\textit{fitness} e \textit{precision}. Essa abordagem é
particularmente útil em ambientes dinâmicos, onde a estrutura do
processo pode mudar com frequência, como na área hospitalar e em
sistemas logísticos.

Ferronato \cite{ferronato2021pm2sim} propõe no framework
\textit{PM2Sim} um processo automatizado de geração de logs
sintéticos a partir de modelos descobertos via mineração de
processos. O sistema transforma o modelo minerado — geralmente
representado como uma Rede de Petri — em um modelo de simulação
implementado em Python por meio da biblioteca \texttt{SimPy}. Durante
a execução da simulação, eventos são registrados em formato XES
(\textit{eXtensible Event Stream}), mantendo a compatibilidade com
ferramentas de mineração como PM4Py e ProM. Essa estratégia permite a
retroalimentação do ciclo de mineração, criando uma integração
contínua entre descoberta, simulação e validação.

Augusto et al. \cite{augusto2016evaluation} reforçam essa importância
ao aplicar a integração entre mineração de processos e simulação em
um contexto clínico. Os autores desenvolveram uma metodologia para
simular fluxos de pacientes com base em logs hospitalares nacionais,
combinando mineração e simulação. Essa abordagem permitiu a criação
de logs sintéticos de trajetórias clínicas, utilizados para avaliar o
impacto de decisões médicas e políticas de gestão sobre taxas de
mortalidade e custos. Os resultados evidenciam o potencial da geração
de logs sintéticos como ferramenta de experimentação em ambientes de
alta complexidade e variabilidade.

A geração de dados artificiais também desempenha um papel importante
na análise de eficiência operacional de ambientes hospitalares. O
estudo de Protil et al. \cite{protil2004taxa} sobre a ocupação de
centros cirúrgicos demonstrou que o uso de modelos simulados
possibilita identificar desperdícios de tempo e gargalos no
agendamento de cirurgias. Embora o trabalho não empregue mineração de
processos, ele antecipa a importância da simulação como meio de
geração de dados para planejamento e otimização de recursos — uma
função equivalente à dos logs sintéticos em contextos modernos.

Outro aspecto relevante é a qualidade dos logs gerados. Conforme
Kherbouche et al. \cite{betterlogs2020}, a utilidade dos logs
sintéticos depende da fidelidade com que reproduzem as
características estatísticas, temporais e comportamentais dos
registros reais. Logs incompletos, redundantes ou inconsistentes
podem comprometer a avaliação do modelo.

A mineração fornece o modelo descritivo; a simulação, o ambiente de
experimentação; e os logs sintéticos, o instrumento de validação.
Esse ciclo permite aprimorar continuamente o modelo de processo,
ajustando suas propriedades comportamentais e estatísticas com base
em dados observados e simulados. Resumidamente, a geração de logs
sintéticos transforma o modelo minerado em uma ferramenta viva de
aprendizado e decisão, capaz de antecipar cenários e apoiar a
otimização operacional em tempo reduzido.

\section{Indicadores de Desempenho em Simulação}

A avaliação de desempenho é um componente essencial em sistemas que utilizam simulação de eventos discretos para otimização de processos. Em simulação, indicadores tradicionais incluem tempo de ciclo (\textit{lead time}), utilização de recursos, capacidade (\textit{throughput}) e eficiência operacional. Essas métricas permitem comparar cenários alternativos e validar se mudanças estruturais geram os ganhos esperados.

No contexto hospitalar específico, Souza, Vaccaro e Lima \cite{souza2020ore} propuseram o indicador \textit{Operating Room Effectiveness} (ORE), adaptado do conceito de \textit{Overall Equipment Effectiveness} (OEE) da manufatura enxuta. O ORE é calculado pelo produto de três dimensões — Disponibilidade, Desempenho e Qualidade — permitindo decompor perdas operacionais em categorias mensuráveis: falhas de equipamento, setup, variações de tempo, cancelamentos e reintervenções.

O ORE foi concebido para fornecer uma visão integrada da efetividade
operacional de salas cirúrgicas, permitindo identificar e classificar
perdas operacionais de forma sistemática. Diferentemente de
indicadores isolados como taxa de ocupação, tempo de cancelamentos ou
resultados financeiros, o ORE oferece uma métrica global que
correlaciona múltiplas dimensões do desempenho operacional.

A base conceitual do ORE provém do OEE, desenvolvido por Nakajima
\cite{nakajima1988introduction} no contexto da Manutenção Produtiva
Total (TPM). O OEE é calculado pelo produto de três índices:
Disponibilidade, Desempenho e Qualidade. Essa estrutura
multiplicativa permite identificar onde ocorrem as principais perdas
do sistema produtivo. A adaptação para o contexto hospitalar mantém
essa estrutura, mas reinterpreta as perdas considerando as
especificidades do fluxo cirúrgico.

A fórmula geral do ORE é expressa por:

\begin{equation}
	\text{ORE} = \text{Disponibilidade} \times \text{Desempenho} \times \text{Qualidade}
\end{equation}

Alternativamente, o ORE pode ser calculado como:

\begin{equation}
	\text{ORE} = \frac{\text{Tempo Total de Valor Agregado (TTAV)}}{\text{Tempo Total Disponível (TTA)}}
\end{equation}

Cada componente do indicador é detalhado nas subseções seguintes.

\subsection{Dimensões e Perdas Operacionais}

O ORE classifica as perdas operacionais em três classes principais,
totalizando sete tipos distintos de perdas. Essa classificação é
fundamental para direcionar ações de melhoria de forma precisa e
mensurável.

\subsubsection{Perdas de Disponibilidade}

A Disponibilidade mede a fração do tempo programado que efetivamente
foi utilizado para atividades cirúrgicas, descontando perdas
relacionadas ao planejamento e preparação das salas. É calculada
como:

\begin{equation}
	\text{Disponibilidade} = \frac{\text{Tempo Total Programado (TTS)}}{\text{Tempo Total Disponível (TTA)}}
\end{equation}

As perdas de disponibilidade incluem:

\begin{itemize}
	\item \textbf{Falha de Equipamento}: tempo de inatividade causado
	      por quebras ou manutenções não programadas de equipamentos cirúrgicos,
	      anestésicos ou de suporte.

	\item \textbf{Setup e Paradas Programadas}: tempo necessário para
	      limpeza, preparação e esterilização da sala entre procedimentos. Este
	      tempo é estrutural e inevitável, mas pode ser otimizado através de
	      padronização e melhoria de processos.

	\item \textbf{Não Agendamento}: tempo ocioso em turnos reservados a
	      cirurgiões específicos, mas sem cirurgias programadas. Esta perda é
	      particularmente relevante em hospitais universitários onde há
	      alocação pré-definida de blocos cirúrgicos por equipe médica.
\end{itemize}

\subsubsection{Perdas de Desempenho}

O Desempenho mede a fração do tempo programado que foi efetivamente
utilizado, descontando variações em relação ao planejado e
cancelamentos. É calculado como:

\begin{equation}
	\text{Desempenho} = \frac{\text{Tempo Total Utilizado (TTU)}}{\text{Tempo Total Programado (TTS)}}
\end{equation}

As perdas de desempenho incluem:

\begin{itemize}
	\item \textbf{Pequenas Paradas}: interrupções momentâneas durante o
	      procedimento cirúrgico causadas por problemas com equipamentos,
	      suprimentos, falta de materiais ou quedas de energia.

	\item \textbf{Variação no Tempo Cirúrgico}: diferenças entre o
	      tempo planejado e o tempo real de execução do procedimento. Cirurgias
	      que excedem significativamente o tempo previsto podem causar
	      cancelamentos em cascata das cirurgias subsequentes.

	\item \textbf{Cancelamentos}: cirurgias canceladas após o
	      agendamento resultam em capacidade ociosa não recuperável. Cerca de 50\% dos cancelamentos ocorrem por falta de
	      condição clínica do paciente ou absenteísmo, indicando falhas na
	      gestão do fluxo pré-operatório.
\end{itemize}

\subsubsection{Perdas de Qualidade}

A Qualidade mede a fração do tempo utilizado que efetivamente agregou
valor ao paciente, descontando retrabalhos e reintervenções. É
calculada como:

\begin{equation}
	\text{Qualidade} = \frac{\text{Tempo Total de Valor Agregado (TTAV)}}{\text{Tempo Total Utilizado (TTU)}}
\end{equation}

As perdas de qualidade incluem:

\begin{itemize}
	\item \textbf{Reintervenções Cirúrgicas}: procedimentos que
	      necessitaram ser refeitos devido a falhas técnicas, complicações
	      evitáveis ou erros no procedimento inicial. Este tipo de perda é
	      difícil de mensurar devido à complexidade de classificar
	      reintervenções como evitáveis ou inerentes à evolução clínica do
	      paciente.
\end{itemize}

\subsection{ORE como Driver de Simulação}

O uso do ORE transcende a função de medição passiva de desempenho. No
contexto de simulação de processos, o ORE atua como um
\textbf{indicador direcionador} que permite:

\begin{enumerate}
	\item \textbf{Identificação de Oportunidades}: a decomposição do
	      ORE nas sete categorias de perdas permite priorizar quais aspectos do
	      processo apresentam maior potencial de ganho.

	\item \textbf{Definição de Metas}: gestores podem estabelecer metas
	      específicas de melhoria para cada dimensão do ORE (por exemplo,
	      "reduzir cancelamentos em 30\%" ou "reduzir tempo de setup em 5
	      minutos").

	\item \textbf{Simulação de Cenários}: modelos de simulação podem
	      ser parametrizados para alcançar valores-alvo de ORE, permitindo
	      avaliar quais configurações operacionais (alocação de recursos,
	      mudanças no cronograma, redução de variabilidade) são necessárias
	      para atingir as metas estabelecidas.

	\item \textbf{Ciclo de Melhoria Contínua}: a medição periódica do
	      ORE permite avaliar o impacto de intervenções e ajustar estratégias
	      de otimização de forma iterativa.
\end{enumerate}

Souza et al. \cite{souza2020ore} demonstraram que cenários
hipotéticos de melhoria, quando simulados, podem estimar com precisão
o impacto de intervenções antes de sua implementação. Por exemplo, a
redução de 50\% nos cancelamentos combinada com eliminação de tempos
ociosos por não agendamento resultou em um ganho projetado de 31,6\%
no ORE, equivalente a aproximadamente 400 horas adicionais mensais de
capacidade cirúrgica.

Os resultados apresentados por Souza et al. \cite{souza2020ore}
demonstraram ganhos de eficiência de 12\% e economias anuais
estimadas em US\$400.000 após a implementação do indicador e de ações
de melhoria em um hospital universitário brasileiro. Tais resultados
evidenciam o potencial do ORE como métrica de apoio à gestão
operacional e à tomada de decisão em ambientes de alta complexidade.

Em trabalhos anteriores, Protil et al. \cite{protil2004taxa} já
haviam explorado o uso de modelagem e simulação de sistemas para
analisar a taxa de ocupação de centros cirúrgicos, apontando a
importância da simulação como ferramenta para otimização de recursos
hospitalares. A integração entre indicadores como o ORE e abordagens
baseadas em mineração e simulação, conforme sugerido por Ferronato
\cite{ferronato2021pm2sim}, amplia a capacidade analítica desses
sistemas, permitindo correlacionar métricas de eficiência com o
comportamento real dos processos.

Dessa forma, o uso combinado de indicadores operacionais como o ORE e
modelos minerados fornece uma visão quantitativa e dinâmica da
eficiência hospitalar, permitindo avaliar e prever o impacto de
decisões sobre produtividade, custos e qualidade dos serviços de
saúde.

\section{Padrão XES (eXtensible Event Stream)}

O padrão \textit{eXtensible Event Stream} (XES) foi desenvolvido pela
\textit{IEEE Task Force on Process Mining} com o objetivo de
padronizar a representação de logs de eventos utilizados em mineração
de processos. Formalizado pela norma IEEE 1849-2016
\cite{ieeexes2010}, o XES define uma estrutura extensível e
interoperável que permite armazenar e trocar informações sobre
execuções de processos entre diferentes ferramentas e plataformas.

Cada log XES é composto por um conjunto de \textit{traces}, que
representam casos individuais de execução, e cada \textit{trace}
contém uma sequência ordenada de \textit{events}, correspondentes às
atividades executadas. Cada evento possui atributos obrigatórios —
como nome da atividade, identificador do caso e carimbo de tempo — e
opcionais, como recursos, custos ou anotações adicionais. Essa
estrutura hierárquica assegura a consistência semântica dos dados e
permite análises multi-perspectiva (fluxo, tempo e organização).

O Código~\ref{fig:xes-exemplo} apresenta um trecho simplificado de um
log XES que segue o padrão IEEE, ilustrando a estrutura de um
\textit{trace} (caso) com três eventos correspondentes a um processo
de atendimento hospitalar.

\begin{lstlisting}[language=XML, caption={Exemplo simplificado de log XES}, label={fig:xes-exemplo}]
<?xml version="1.0" encoding="UTF-8"?>
<log xes.version="1.0" xes.features="nested-attributes"
     xmlns="http://www.xes-standard.org/">
  <trace>
    <string key="concept:name" value="Case001"/>
    <event>
      <string key="concept:name" value="Admissao do Paciente"/>
      <date key="time:timestamp" value="2024-03-15T08:30:00.000+00:00"/>
      <string key="org:resource" value="Recepcao"/>
    </event>
    <event>
      <string key="concept:name" value="Avaliacao Medica"/>
      <date key="time:timestamp" value="2024-03-15T09:00:00.000+00:00"/>
      <string key="org:resource" value="Dr. Silva"/>
    </event>
    <event>
      <string key="concept:name" value="Alta Hospitalar"/>
      <date key="time:timestamp" value="2024-03-15T10:15:00.000+00:00"/>
      <string key="org:resource" value="Administracao"/>
    </event>
  </trace>
</log>
\end{lstlisting}

Nesse exemplo, o caso \texttt{Case001} representa a trajetória de um
paciente desde a admissão até a alta hospitalar. Cada evento contém
informações sobre a atividade executada (\textit{concept:name}), o
horário em que ocorreu (\textit{time:timestamp}) e o recurso
responsável (\textit{org:resource}). A simplicidade e a
extensibilidade do formato permitem que diferentes sistemas coletem e
exportem dados compatíveis para posterior mineração.

% ----------------------------------------------------------
% Metodologia
% ----------------------------------------------------------
