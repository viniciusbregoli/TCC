\chapter{Metodologia}

Este capítulo apresenta a metodologia utilizada para o
desenvolvimento do gerador de modelos de simulação baseado em
mineração de processos. A abordagem metodológica foi estruturada em
quatro etapas principais: análise automática de logs, mineração de
processos, simulação de eventos discretos e validação de qualidade.

\section{Visão Geral da Abordagem Metodológica}

A metodologia proposta consiste em um pipeline sequencial de quatro
etapas interdependentes, conforme ilustrado na Figura

\begin{enumerate}
	\item \textbf{Análise Automática de Logs}: detecção de atributos e validação de compatibilidade
	\item \textbf{Mineração de Processos}: extração do modelo formal e parâmetros estatísticos
	\item \textbf{Simulação de Logs Sintéticos}: geração de casos baseada em eventos discretos
	\item \textbf{Validação de Qualidade}: avaliação de similaridade e conformidade
\end{enumerate}

\section{Arquitetura do Sistema}

\subsection{Filosofia Arquitetural}

O Sim2Log-Core foi projetado seguindo uma \textbf{arquitetura em
	camadas modular} com clara separação de responsabilidades. Esta
abordagem arquitetural fundamenta-se em princípios de engenharia de
software estabelecidos, incluindo Separação de Responsabilidades
(\textit{Separation of Concerns}), Princípio da Responsabilidade Única
(\textit{Single Responsibility Principle}) e Inversão de Dependências
(\textit{Dependency Inversion}). A arquitetura foi estruturada em cinco
camadas hierárquicas, conforme ilustrado na
Figura~\ref{fig:arquitetura_camadas}.

\begin{figure}[htb]
	\centering
	\fbox{\begin{minipage}{0.95\textwidth}
			\small
			\begin{verbatim}
┌────────────────────────────────────────────────────────┐
│           CAMADA 1: INTERFACE DE USUÁRIO               │
│  ┌──────────────────────────────────────────────────┐  │
│  │  Streamlit Web Dashboard (app/app.py)            │  │
│  │  - Upload de arquivos XES                        │  │
│  │  - Execução passo-a-passo                        │  │
│  │  - Visualizações interativas                     │  │
│  │  - Exportação de resultados                      │  │
│  └──────────────────────────────────────────────────┘  │
└───────────────────────┬────────────────────────────────┘
                        │
                        ▼
┌────────────────────────────────────────────────────────┐
│              CAMADA 2: FACHADA                         │
│  ┌──────────────────────────────────────────────────┐  │
│  │  ProcessMiningPipeline (pipeline.py)             │  │
│  │  - Orquestra todos os componentes                │  │
│  │  - Fornece API de alto nível                     │  │
│  │  - Gerencia fluxo de dados entre etapas          │  │
│  │  - Método: run_full_pipeline()                   │  │
│  └──────────────────────────────────────────────────┘  │
└───────────┬────┬────┬────┬────┬───────────────────────┘
            │    │    │    │    │
            ▼    ▼    ▼    ▼    ▼
┌────────────────────────────────────────────────────────┐
│        CAMADA 3: COMPONENTES (Serviços)               │
├──────────┬──────────┬──────────┬──────────┬───────────┤
│LogAnalyz │ProcessM- │LogSimul- │LogValid- │ORECalc-   │
│er        │iner      │ator      │ator      │ulator     │
│(396 LOC) │(459 LOC) │(334 LOC) │(118 LOC) │(451 LOC)  │
│          │          │          │          │           │
│Perfila-  │Descobre  │Gera logs │Valida    │Calcula    │
│mento     │modelos   │sintéti-  │qualidade │métricas   │
│automático│formais   │cos       │          │ORE        │
└──────────┴──────────┴──────────┴──────────┴───────────┘
            │
            ▼
┌────────────────────────────────────────────────────────┐
│       CAMADA 4: MODELOS DE DADOS (models.py)           │
│  - SimulationConfig      - ProcessModel                │
│  - ActivityStatistics    - SimulationResult            │
│  - ValidationResult      - OREMetrics                  │
│  - LogProfile                                          │
└────────────────────────────────────────────────────────┘
            │
            ▼
┌────────────────────────────────────────────────────────┐
│         CAMADA 5: INFRAESTRUTURA                       │
│  PM4Py | SimPy | Pandas | SciPy | NumPy               │
└────────────────────────────────────────────────────────┘
\end{verbatim}
		\end{minipage}}
	\caption{Arquitetura em camadas do Sim2Log-Core. Fonte: Autor (2025)}
	\label{fig:arquitetura_camadas}
\end{figure}

\subsubsection{Camada 1: Interface de Usuário}

Esta camada fornece o ponto de acesso para usuários finais através de
um dashboard web interativo implementado com Streamlit. A interface
permite operações de upload, configuração, execução e visualização de
resultados sem necessidade de programação.

\subsubsection{Camada 2: Fachada}

Implementa o padrão de projeto Facade através da classe
\texttt{ProcessMiningPipeline}, que fornece uma interface simplificada
e unificada para o subsistema complexo. Esta camada oculta a
complexidade da coordenação entre os cinco componentes independentes,
oferecendo métodos de alto nível como \texttt{run\_full\_pipeline()} e
\texttt{quick\_analysis()}.

\subsubsection{Camada 3: Componentes}

Cada componente é uma unidade independente e testável que implementa
uma responsabilidade específica do sistema. Esta arquitetura baseada em
componentes permite:

\begin{itemize}
	\item \textbf{Modularidade}: componentes podem ser usados independentemente
	\item \textbf{Testabilidade}: cada componente possui testes isolados
	\item \textbf{Manutenibilidade}: mudanças em um componente não afetam outros
	\item \textbf{Extensibilidade}: novos componentes podem ser adicionados sem modificar existentes
\end{itemize}

\subsubsection{Camada 4: Modelos de Dados}

Define estruturas de dados fortemente tipadas usando Python dataclasses,
garantindo type safety e auto-documentação. Estas classes servem como
contratos de interface entre os componentes.

\subsubsection{Camada 5: Infraestrutura}

Bibliotecas de terceiros que fornecem funcionalidades fundamentais de
mineração de processos, simulação, análise estatística e manipulação de
dados.

\subsection{Padrões de Projeto Utilizados}

\subsubsection{Padrão Facade (Fachada)}

A classe \texttt{ProcessMiningPipeline} implementa o padrão Facade
\cite{gamma1994design}, fornecendo uma interface unificada e
simplificada para o subsistema complexo de cinco componentes
independentes.

\textbf{Problema}: A utilização direta dos componentes individuais
requer conhecimento detalhado de suas interfaces, dependências e ordem
de execução, aumentando a complexidade para o usuário.

\textbf{Solução}: O facade encapsula a complexidade da orquestração,
oferecendo métodos simples:

\begin{verbatim}
# Interface simplificada
pipeline = ProcessMiningPipeline()
results = pipeline.run_full_pipeline("log.xes")

# Em vez de:
# analyzer = LogAnalyzer()
# profile = analyzer.analyze("log.xes")
# miner = ProcessMiner()
# model = miner.mine_process("log.xes")
# simulator = LogSimulator(config)
# result = simulator.simulate(model)
# validator = LogValidator()
# validation = validator.validate("original.xes", "synthetic.xes")
\end{verbatim}

\textbf{Benefícios}:
\begin{itemize}
	\item Redução da complexidade para o código cliente
	\item Desacoplamento entre cliente e implementação interna
	\item Facilita a orquestração do workflow completo
	\item Permite evolução interna sem quebrar código cliente
\end{itemize}

\subsubsection{Arquitetura Baseada em Componentes}

Cada componente (\texttt{LogAnalyzer}, \texttt{ProcessMiner},
\texttt{LogSimulator}, \texttt{LogValidator}, \texttt{ORECalculator}) é
projetado para ser independente e reutilizável:

\begin{verbatim}
# Cada componente pode ser usado isoladamente
analyzer = LogAnalyzer()
profile = analyzer.analyze("log.xes")

miner = ProcessMiner()
model = miner.mine_process("log.xes")

simulator = LogSimulator(config)
result = simulator.simulate(model)
\end{verbatim}

\textbf{Benefícios}:
\begin{itemize}
	\item Modularidade e coesão alta
	\item Acoplamento baixo entre componentes
	\item Testabilidade independente
	\item Facilita manutenção e evolução
\end{itemize}

\subsubsection{Data Classes para Type Safety}

O sistema utiliza Python dataclasses com type hints para garantir
segurança de tipos e auto-documentação:

\begin{verbatim}
@dataclass
class ProcessModel:
    petri_net: object
    initial_marking: object
    final_marking: object
    activities: Dict[str, ActivityStatistics]
    arrival_rate: float
    median_case_duration: float
    quality_metrics: Dict[str, float]
\end{verbatim}

\textbf{Benefícios}:
\begin{itemize}
	\item Type hints para suporte de IDEs
	\item Código auto-documentado
	\item Validação em tempo de desenvolvimento
	\item Serialização facilitada
\end{itemize}

\subsubsection{Padrão Strategy para Ajuste de Distribuições}

O ajuste de distribuições estatísticas implementa o padrão Strategy,
testando múltiplas estratégias (Normal, Log-Normal, Exponencial) e
selecionando a melhor através do teste de Kolmogorov-Smirnov:

\begin{verbatim}
# Tenta Normal → Log-Normal → Exponencial
# Seleciona melhor via teste KS (maior p-valor)
best_fit = fitter.fit(durations)
\end{verbatim}

\subsection{Arquitetura de Classes}

A Figura~\ref{fig:class_architecture} apresenta a arquitetura detalhada
das classes principais do sistema, incluindo seus atributos, métodos e
relacionamentos.

\begin{figure}[htb]
	\centering
	\fbox{\begin{minipage}{0.95\textwidth}
			\small
			\begin{verbatim}
┌──────────────────────────────────────────────────────┐
│         ProcessMiningPipeline (402 LOC)              │
├──────────────────────────────────────────────────────┤
│ Attributes:                                          │
│ - verbose: bool                                      │
│ - daily_hours: float                                 │
│ - setup_time_minutes: float                          │
│ - analyzer: LogAnalyzer                              │
│ - miner: ProcessMiner                                │
│ - simulator: LogSimulator                            │
│ - validator: LogValidator                            │
│ - ore_calculator: ORECalculator                      │
├──────────────────────────────────────────────────────┤
│ Methods:                                             │
│ + analyze_log(log_path: str) -> LogProfile           │
│ + mine_process(log_path, filter) -> ProcessModel     │
│ + simulate(model, num_cases) -> SimulationResult     │
│ + validate(original, simulated) -> ValidationResult  │
│ + calculate_ore(log_path) -> OREMetrics              │
│ + run_full_pipeline(log_path) -> Dict                │
└──────────────────────────────────────────────────────┘
         │ uses
         ├────────────────┬────────────┬─────────────┐
         ▼                ▼            ▼             ▼
┌─────────────────┐ ┌──────────────┐ ┌──────────┐ ┌──────────────┐
│  LogAnalyzer    │ │ProcessMiner  │ │LogSimul- │ │LogValidator  │
│  (396 LOC)      │ │(459 LOC)     │ │ator      │ │(118 LOC)     │
├─────────────────┤ ├──────────────┤ │(334 LOC) │ ├──────────────┤
│+ analyze()      │ │+ mine_proc-  │ ├──────────┤ │+ validate()  │
│  -> LogProfile  │ │  ess()       │ │+ simula- │ │  -> Valid-   │
│                 │ │  -> Process  │ │  te()    │ │     ation-   │
│                 │ │     Model    │ │  -> Sim- │ │     Result   │
│                 │ │              │ │     ula- │ │              │
│                 │ │              │ │     tion │ │              │
│                 │ │              │ │     Res- │ │              │
│                 │ │              │ │     ult  │ │              │
└─────────────────┘ └──────────────┘ └──────────┘ └──────────────┘
         │ produces       │ produces    │ produces   │ produces
         ▼                ▼             ▼            ▼
┌─────────────────────────────────────────────────────────┐
│              DATA MODELS (models.py - 131 LOC)          │
├──────────────┬──────────────┬──────────────┬───────────┤
│ LogProfile   │ProcessModel  │Simulation    │Validation │
│              │              │Result        │Result     │
├──────────────┼──────────────┼──────────────┼───────────┤
│- num_traces  │- petri_net   │- csv_path    │- fitness  │
│- num_events  │- activities  │- xes_path    │- cost     │
│- activity_   │- arrival_    │- num_cases_  │- similar- │
│  frequencies │  rate        │  generated   │  ity_%    │
│- variants    │- quality_    │- timestamp   │- details  │
│              │  metrics     │              │           │
└──────────────┴──────────────┴──────────────┴───────────┘
         │
         ├─────────────────────────────┐
         │                             │
         ▼                             ▼
┌──────────────────────┐    ┌──────────────────────┐
│ ActivityStatistics   │    │    OREMetrics        │
├──────────────────────┤    ├──────────────────────┤
│- min_duration        │    │- ore: float          │
│- max_duration        │    │- availability        │
│- mean_duration       │    │- performance         │
│- std_duration        │    │- quality             │
│- distribution:       │    │- loss_breakdown:     │
│  {name, params,      │    │  Dict[str, float]    │
│   p_value}           │    │- cancellation_rate   │
│- frequency           │    │                      │
│- resources: List     │    │                      │
└──────────────────────┘    └──────────────────────┘
\end{verbatim}
		\end{minipage}}
	\caption{Arquitetura de classes do Sim2Log-Core. Fonte: Autor (2025)}
	\label{fig:class_architecture}
\end{figure}

\subsection{Dependências entre Módulos}

A Figura~\ref{fig:module_dependencies} ilustra o grafo de dependências
entre os módulos do sistema:

\begin{figure}[htb]
	\centering
	\fbox{\begin{minipage}{0.95\textwidth}
			\small
			\begin{verbatim}
pipeline.py (Orquestrador)
├──→ log_analyzer.py
├──→ process_mining.py
├──→ simulation.py
├──→ validation.py
├──→ ore_indicators.py
└──→ models.py

process_mining.py
├──→ models.py
├──→ utils.py
└──→ [PM4Py, SciPy]

simulation.py
├──→ models.py
├──→ utils.py
└──→ [SimPy, PM4Py, Pandas]

ore_indicators.py
├──→ models.py
├──→ utils.py
└──→ [Pandas, SciPy]

validation.py
├──→ models.py
└──→ [PM4Py, Pandas]

log_analyzer.py
├──→ models.py
└──→ [PM4Py, Pandas]

app/app.py (Interface Web)
└──→ pipeline.py
    └──→ (Todos os componentes)
\end{verbatim}
		\end{minipage}}
	\caption{Grafo de dependências entre módulos. Fonte: Autor (2025)}
	\label{fig:module_dependencies}
\end{figure}

\textbf{Observações sobre as dependências}:

\begin{itemize}
	\item \textbf{Acoplamento unidirecional}: Dependências fluem sempre para baixo na hierarquia
	\item \textbf{Módulo models.py como contrato}: Todos os componentes dependem apenas dos modelos de dados, não uns dos outros
	\item \textbf{Componentes independentes}: LogAnalyzer, ProcessMiner, LogSimulator, LogValidator e ORECalculator não se conhecem mutuamente
	\item \textbf{Pipeline como único ponto de acoplamento}: Apenas o pipeline conhece todos os componentes
\end{itemize}

Esta estrutura permite evolução independente de cada componente sem
efeitos colaterais, facilita testes unitários isolados e permite
substituição de implementações sem afetar o resto do sistema.

\subsection{Fluxo de Dados entre Componentes}

A Figura~\ref{fig:data_flow_components} ilustra como os objetos de
dados fluem entre os componentes:

\begin{figure}[htb]
	\centering
	\fbox{\begin{minipage}{0.95\textwidth}
			\small
			\begin{verbatim}
INPUT: log.xes (arquivo XES)
  │
  ▼
┌─────────────────────────┐
│   LogAnalyzer           │
│   analyze(log.xes)      │
└───────────┬─────────────┘
            │ produz
            ▼
      ┌──────────────┐
      │ LogProfile   │ ─────┐
      └──────────────┘      │ alimenta
            │               │
            ▼               │
┌─────────────────────────┐ │
│   ProcessMiner          │ │
│   mine_process(log.xes) │◄┘
└───────────┬─────────────┘
            │ produz
            ▼
      ┌──────────────┐
      │ProcessModel  │
      │- petri_net   │
      │- activities  │
      │- statistics  │
      └──────┬───────┘
             │ alimenta
             ▼
┌──────────────────────────┐
│   LogSimulator           │
│   simulate(model)        │
└───────────┬──────────────┘
            │ produz
            ▼
      ┌──────────────────┐
      │SimulationResult  │
      │- csv_path        │
      │- xes_path        │
      └────┬─────────────┘
           │ alimenta
           ▼
┌────────────────────────────┐
│   LogValidator             │
│   validate(original,       │
│            synthetic)      │
└───────────┬────────────────┘
            │ produz
            ▼
      ┌──────────────────┐
      │ValidationResult  │
      │- fitness         │
      │- similarity      │
      └──────────────────┘

   (Paralelamente)
INPUT: log.xes
  │
  ▼
┌─────────────────────────┐
│   ORECalculator         │
│   calculate_from_log()  │
└───────────┬─────────────┘
            │ produz
            ▼
      ┌──────────────┐
      │  OREMetrics  │
      │  - ore       │
      │  - losses    │
      └──────────────┘
\end{verbatim}
		\end{minipage}}
	\caption{Fluxo de dados entre componentes. Fonte: Autor (2025)}
	\label{fig:data_flow_components}
\end{figure}

\subsection{Princípios de Design Seguidos}

O desenvolvimento do Sim2Log-Core fundamentou-se em princípios
estabelecidos de engenharia de software:

\begin{table}[htb]
	\centering
	\caption{Princípios de design aplicados no sistema}
	\label{tab:design_principles}
	\begin{tabular}{p{4cm}p{8cm}}
		\hline
		\textbf{Princípio}                             & \textbf{Implementação no Sistema}                                                                           \\
		\hline
		\textbf{Separação de Responsabilidades}        & Cada módulo tem responsabilidade única e bem definida                                                       \\
		\textbf{DRY (Don't Repeat Yourself)}           & Funções comuns centralizadas no módulo utils.py                                                             \\
		\textbf{Aberto/Fechado}                        & Fácil extensão sem modificar código existente (novos componentes podem ser adicionados)                     \\
		\textbf{Inversão de Dependências}              & Componentes dependem de abstrações (dataclasses) não de implementações concretas                            \\
		\textbf{SOLID}                                 & Responsabilidade única, injeção de dependências, segregação de interfaces                                  \\
		\textbf{Fail-Fast}                             & Validações no início de cada método, lançamento de exceções descritivas                                     \\
		\textbf{Composition over Inheritance}          & Pipeline compõe componentes em vez de herança                                                               \\
		\textbf{Single Source of Truth}                & models.py define estruturas de dados usadas por todos                                                       \\
		\textbf{Reproducibility}                       & Seed fixo, versionamento de dependências, logging completo                                                  \\
		\textbf{Graceful Degradation}                  & Sistema funciona mesmo com dados parciais (ex: recursos opcionais, atributos enriquecidos opcionais)        \\
		\hline
	\end{tabular}
\end{table}

\section{Fluxo de Dados Detalhado}

A Figura~\ref{fig:fluxo_metodologia} ilustra o fluxo completo de
dados através do sistema:

\begin{figure}[htb]
	\centering
	\includegraphics[width=\textwidth,height=0.85\textheight,keepaspectratio]{figuras/fig2.png}
	\caption{Fluxo completo da metodologia. Fonte: Autor (2025)}
	\label{fig:fluxo_metodologia}
\end{figure}
\clearpage

Cada etapa produz artefatos específicos que alimentam a etapa
seguinte, garantindo rastreabilidade e reprodutibilidade do processo.
O sistema foi projetado para ser 100\% genérico, funcionando com logs
de qualquer domínio organizacional que disponha de registros
estruturados em formato XES.

\section{Etapa 1: Análise Automática de Logs}

\subsection{Diagrama de Componente}

A Figura~\ref{fig:fase1_componente} apresenta o diagrama de
componente da Etapa 1. O componente central, \texttt{LogAnalyzer}, é
responsável por realizar a análise automática e a extração de perfil
de um log de eventos.

O componente recebe como entrada um \textbf{log de eventos} no
formato XES, que encapsula os traces, eventos, atributos, timestamps
e recursos do processo. A análise é parametrizada por uma
\textbf{configuração} que define o modo de operação, como a ativação
da auto-detecção de atributos e a validação do formato.

Internamente, o \texttt{LogAnalyzer} utiliza \textbf{métodos de
	análise} baseados em bibliotecas como PM4Py para realizar a detecção
de padrões, análise de variantes e análise estatística. As principais
tarefas executadas são a detecção automática da estrutura do log, a
identificação das variantes do processo, a validação de
compatibilidade com as etapas subsequentes e a extração de
estatísticas descritivas.

Como resultado, o componente produz um \textbf{Perfil do Log}
(\texttt{LogProfile}), um artefato estruturado que contém as
características detectadas. Isso inclui métricas quantitativas
(número de casos, eventos, atividades e recursos únicos), estruturais
(variantes de processo, nomes dos atributos-chave identificados) e
temporais (estatísticas de duração e frequência), além de uma
verificação de compatibilidade para o pipeline.

\begin{figure}[htb]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.25\textwidth, keepaspectratio]{figuras/diagrama1.png}}
	\caption{Fase 1: Análise Automática de Logs. Fonte: Autor (2025)}
	\label{fig:fase1_componente}
\end{figure}

\subsection{Objetivo}

Detectar automaticamente as características estruturais, temporais e
operacionais do log de entrada, incluindo o cálculo de indicadores de
eficiência operacional quando aplicável, assegurando compatibilidade
com diferentes domínios e formatos sem necessidade de configuração
manual.

\subsection{Justificativa}

Logs de eventos variam significativamente entre domínios quanto à
nomenclatura de atributos, presença de recursos organizacionais e
convenções de registro temporal. A análise automática elimina a
necessidade de parametrização manual e permite processamento
agnóstico ao domínio, ampliando a aplicabilidade do sistema. Além da
análise estrutural, a integração de indicadores de eficiência como o
ORE permite quantificar perdas operacionais e direcionar a simulação
para cenários de melhoria específicos.

\subsection{Detecção de Atributos-Chave}

O sistema implementa um mecanismo de detecção baseado em prioridades,
testando sequencialmente diferentes convenções de nomenclatura até
identificar os atributos obrigatórios:

\begin{itemize}
	\item \textbf{Activity Key} (nome da atividade): \texttt{concept:name}, \texttt{Activity}, \texttt{activity}, \texttt{event}, \texttt{task}
	\item \textbf{Timestamp Key} (momento do evento): \texttt{time:timestamp}, \texttt{timestamp}, \texttt{Time}, \texttt{start\_time}
	\item \textbf{Case ID Key} (identificador do caso): \texttt{concept:name}, \texttt{case\_id}, \texttt{CaseID}, \texttt{Case}
	\item \textbf{Resource Key} (executor - opcional): \texttt{org:resource}, \texttt{resource}, \texttt{user}, \texttt{actor}
\end{itemize}

A ordem de prioridade baseia-se no padrão IEEE XES \cite{ieeexes2010}
e em análise de datasets públicos do BPI Challenge e repositório 4TU.

Quando nenhum candidato da lista de prioridades é encontrado, o
sistema aplica busca por palavras-chave nos nomes dos atributos. Se
ainda assim a detecção falhar para atributos obrigatórios, uma
exceção é lançada com lista dos atributos disponíveis para
diagnóstico.

\subsection{Coleta de Estatísticas Estruturais}

Para cada log analisado, o sistema extrai as seguintes informações:

\begin{itemize}
	\item Número total de casos (traces) e eventos
	\item Número de atividades únicas
	\item Distribuição de frequência das atividades
	\item Comprimento mínimo, máximo e médio dos traces
	\item Número de variantes do processo (sequências únicas de atividades)
	\item Distribuição de recursos por atividade (quando disponível)
\end{itemize}

Essas estatísticas são armazenadas em um objeto \texttt{LogProfile},
que serve como entrada para as etapas posteriores e permite análise
exploratória dos dados antes da mineração.

\subsection{Saída da Etapa}

\noindent Um objeto \texttt{LogProfile} contendo:
\begin{itemize}
	\item \textbf{Metadados estruturais}: número de traces, eventos,
	      atividades únicas, variantes
	\item \textbf{Mapeamento de atributos}: activity\_key, timestamp\_key,
	      case\_id\_key, resource\_key (quando disponível)
	\item \textbf{Estatísticas descritivas}: frequências de atividades,
	      comprimentos de traces, distribuição de variantes
	\item \textbf{Flags de compatibilidade}: has\_resources,
	      has\_lifecycle, has\_enriched\_attributes
	\item \textbf{Informações de recursos}: mapeamento de recursos por
	      atividade (quando disponível)
\end{itemize}

\textbf{Nota sobre Métricas de Eficiência Operacional}: O sistema
possui um componente independente (\texttt{ORECalculator}) para
cálculo de métricas de eficiência operacional específicas para
processos hospitalares. Este componente é descrito em detalhes na
Seção "Componente Independente: Cálculo de Métricas ORE" e pode ser
invocado paralelamente ao pipeline principal quando o log contém
atributos específicos do domínio hospitalar.

\section{Etapa 2: Mineração de Processos}

\subsection{Diagrama de Componente}

A Figura~\ref{fig:fase2_componente} apresenta o diagrama de
componente da Etapa 2, detalhando o processo de mineração de
processos e extração de parâmetros estatísticos.

O componente central \texttt{ProcessMiner} recebe como entradas o
arquivo XES original, o objeto \texttt{LogProfile} produzido na Etapa
1 contendo os metadados do log, o parâmetro \texttt{variant\_filter}
configurado em 0.8 para filtragem de variantes, e uma flag
\texttt{save\_model\_image} para geração opcional de visualização do
modelo. O processamento interno é organizado em cinco fases
sequenciais: (1) filtragem de variantes baseada em frequência,
mantendo apenas as variantes que representam 80\% dos casos totais
para remoção de ruído; (2) descoberta do modelo de processo
utilizando o algoritmo Inductive Miner, que garante propriedades de
soundness e sempre produz um modelo bem-formado; (3) conversão do
Process Tree resultante para uma Rede de Petri formal com marcação
inicial e final; (4) ajuste de distribuições estatísticas (Normal,
Log-Normal e Exponencial) às durações observadas de cada atividade,
utilizando o teste de Kolmogorov-Smirnov para seleção da distribuição
com melhor aderência; (5) avaliação de qualidade do modelo através
das métricas de fitness, precision e simplicity.

A saída produzida consiste em um objeto estruturado
\texttt{ProcessModel} contendo: a Rede de Petri formal representada
pela tupla (\texttt{net}, \texttt{im}, \texttt{fm}), onde
\texttt{net} é a rede, \texttt{im} é a marcação inicial e \texttt{fm}
é a marcação final; um dicionário de \texttt{ActivityStatistics} para
cada atividade, incluindo duração média, desvio padrão, distribuição
ajustada e seus parâmetros; métricas temporais globais como
\texttt{arrival\_rate} (taxa de chegada de casos),
\texttt{dispersion\_rate} e \texttt{median\_duration}; métricas de
qualidade do modelo (\texttt{fitness}, \texttt{precision},
\texttt{simplicity}); mapeamento de recursos por atividade quando
disponível; e o \texttt{LogProfile} original para rastreabilidade. As
bibliotecas PM4Py são utilizadas para implementação do Inductive
Miner e cálculo de métricas de conformidade, enquanto SciPy (módulo
\texttt{scipy.stats}) é empregado para ajuste e avaliação das
distribuições estatísticas.

\begin{figure}[htb]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.25\textwidth, keepaspectratio]{figuras/diagrama2.png}}
	\caption{Fase 2: Mineração de Processos. Fonte: Autor (2025)}
	\label{fig:fase2_componente}
\end{figure}

\subsection{Objetivo}

Extrair o modelo formal do processo na forma de uma Rede de Petri e
parametrizar suas características temporais e organizacionais para
uso na simulação.

\subsection{Filtragem de Variantes}

\subsubsection{Problema}

Logs reais frequentemente contêm ruído, casos excepcionais e
variantes raras que dificultam a descoberta de modelos
representativos e podem levar a overfitting.

\subsubsection{Solução}

Aplicação de filtragem baseada em frequência, mantendo apenas as
variantes que representam um percentual especificado dos casos
totais.

\subsubsection{Parâmetro Padrão}

O sistema utiliza como padrão a retenção de 80\% das variantes mais
frequentes (\texttt{variant\_filter=0.8}).

\subsubsection{Justificativa}

O valor de 80\% fundamenta-se no Princípio de Pareto, onde
aproximadamente 20\% das variantes explicam 80\% do comportamento
observado \cite{aalst2016process}. Este limiar oferece balance
adequado entre cobertura comportamental e remoção de ruído.

\subsection{Descoberta do Modelo de Processo}

\subsubsection{Algoritmo Selecionado}

O sistema utiliza o \textbf{Inductive Miner} proposto por Leemans,
Fahland e van der Aalst \cite{leemans2013discovering}.

\subsubsection{Justificativa da Escolha}

A Tabela~\ref{tab:algoritmos} apresenta comparação entre algoritmos
de descoberta disponíveis:

\begin{table}[htb]
	\centering
	\caption{Comparação entre algoritmos de descoberta de processos}
	\label{tab:algoritmos}
	\begin{tabular}{l>{\raggedright\arraybackslash}p{4cm}>{\raggedright\arraybackslash}p{4cm}>{\raggedright\arraybackslash}p{3cm}}
		\hline
		\textbf{Algoritmo}       & \textbf{Vantagens}                                         & \textbf{Desvantagens}     & \textbf{Decisão}   \\
		\hline
		Alpha Miner              & Simples, pioneiro                                          & Não lida com ruído, loops & Não escolhido      \\
		Heuristic Miner          & Tolerante a ruído                                          & Modelos não-formais       & Não escolhido      \\
		\textbf{Inductive Miner} & Soundness garantido, robusto a ruído, sempre produz modelo & Pode generalizar demais   & \textbf{ESCOLHIDO} \\
		Split Miner              & Alta precisão                                              & Requer tuning complexo    & Não escolhido      \\
		\hline
	\end{tabular}
\end{table}

O Inductive Miner foi escolhido por garantir três propriedades
fundamentais:

\begin{enumerate}
	\item \textbf{Soundness}: modelo sempre bem-formado, sem deadlocks
	\item \textbf{Fitness}: modelo sempre capaz de reproduzir o log
	\item \textbf{Completude}: sempre produz um modelo, mesmo com logs problemáticos
\end{enumerate}

Além disso, o algoritmo gera modelos estruturados em blocos
(block-structured), facilitando a conversão para formatos simuláveis.

\subsubsection{Saída}

Uma Rede de Petri formalmente definida como a tupla $N = (P, T, F)$,
onde:
\begin{itemize}
	\item $P$: conjunto de lugares (places)
	\item $T$: conjunto de transições (transitions)
	\item $F \subseteq (P \times T) \cup (T \times P)$: conjunto de arcos
\end{itemize}

Acompanhada de marcação inicial ($M_0$) e marcação final ($M_f$).

\subsection{Extração de Estatísticas Temporais}

\subsubsection{Cálculo de Durações}

Para cada atividade, o sistema calcula durações baseando-se em:

\textbf{Casos com timestamp de conclusão}:
\begin{equation}
	duration = time:complete - time:timestamp
\end{equation}

\textbf{Casos sem timestamp de conclusão} (maioria dos logs):
\begin{equation}
	duration = timestamp(event_{i+1}) - timestamp(event_i)
\end{equation}

\subsection{Ajuste de Distribuições Estatísticas}

\subsubsection{Objetivo}

Modelar a variabilidade realista das durações para simulação
estocástica fiel ao comportamento observado.

\subsubsection{Distribuições Candidatas}

O sistema testa três distribuições de probabilidade:

\begin{enumerate}
	\item \textbf{Normal (Gaussiana)}: para atividades com duração relativamente constante e variação simétrica
	\item \textbf{Log-Normal}: para atividades com cauda longa à direita, comum em processos humanos
	\item \textbf{Exponencial}: para tempos de espera e processos de chegada
\end{enumerate}

\subsection{Avaliação de Qualidade do Modelo}

O sistema calcula três métricas complementares de qualidade
\cite{aalst2016process}:

\subsubsection{Fitness (0-1, maior é melhor)}

\textbf{Definição}: proporção de comportamento do log que o modelo consegue reproduzir.

\textbf{Método}: replay fitness baseado em alinhamentos.

\textbf{Cálculo}: O fitness utiliza token-based replay ou alignment-based conformance. A fórmula mais comum é:

\begin{equation}
	\text{fitness} = 1 - \frac{\text{custo\_total}}{\text{custo\_máximo\_possível}}
\end{equation}

\textbf{Definição do Custo}: O custo representa o número de operações necessárias para fazer o modelo aceitar um trace do log. Durante a mineração, o custo inclui: (1) \textit{movimentos no modelo} - transições silenciosas que precisam ser executadas, (2) \textit{movimentos no log} - eventos que precisam ser ignorados, e (3) \textit{movimentos síncronos} - eventos que coincidem perfeitamente (custo zero). Algoritmos de mineração usam esse custo para decidir quando parar de dividir o log e qual divisão produz o melhor modelo.

No contexto de alinhamentos, para cada trace individual:
\begin{equation}
	\text{fitness\_trace}_i = 1 - \frac{\text{edit\_distance}_i}{\text{max\_length}_i}
\end{equation}

O fitness global do log é calculado como:
\begin{equation}
	\text{fitness\_log} = \frac{1}{n} \sum_{i=1}^{n} \text{fitness\_trace}_i
\end{equation}

onde $n$ é o número total de traces no log. O PM4Py implementa
automaticamente esses cálculos utilizando algoritmos de alinhamento
otimizados.

\textbf{Interpretação}:
\begin{itemize}
	\item $\geq 0.9$: modelo explica quase todo o log
	\item $0.7-0.9$: modelo explica maior parte
	\item $< 0.7$: modelo inadequado
\end{itemize}

\subsubsection{Precision (0-1, maior é melhor)}

\textbf{Definição}: quão preciso é o modelo, evitando comportamento extra não observado.

\textbf{Método}: token-based conformance.

\textbf{Cálculo}: A precision mede quantos comportamentos extras (não observados) o modelo permite. Utiliza a fórmula ETC (Escaping Arcs):

\begin{equation}
	\text{precision} = 1 - \frac{\text{arcos\_escapando}}{\text{arcos\_totais}}
\end{equation}

onde os arcos escapando representam transições que podem ser
executadas pelo modelo mas não foram observadas no log original. O
PM4Py calcula automaticamente essa métrica através de algoritmos de
conformance checking que identificam comportamentos não observados.

\textbf{Interpretação}:
\begin{itemize}
	\item $\geq 0.8$: modelo preciso
	\item $< 0.5$: modelo muito generalista
\end{itemize}

\subsubsection{Simplicity (0-1, maior é melhor)}

\textbf{Definição}: simplicidade estrutural do modelo.

\textbf{Método}: razão entre arcos e nós na rede de Petri.

\textbf{Cálculo}: A simplicidade mede a complexidade topológica da Rede de Petri através da fórmula estrutural:

\begin{equation}
	\text{simplicity} = \frac{1}{1 + \frac{|\text{arcos}|}{|\text{nós}|}}
\end{equation}

onde $|\text{arcos}|$ representa o número total de arcos (transições)
e $|\text{nós}|$ representa o número total de nós (places) na rede de
Petri. O PM4Py calcula automaticamente essa métrica analisando a
estrutura topológica do modelo descoberto.

\textbf{Interpretação}:
\begin{itemize}
	\item $\geq 0.7$: modelo simples
	\item $< 0.4$: modelo complexo
\end{itemize}

\subsection{Saída da Etapa}

\noindent Um objeto \texttt{ProcessModel} contendo:
\begin{itemize}
	\item Rede de Petri (net, im, fm)
	\item Dicionário de estatísticas de atividades
	      (\texttt{ActivityStatistics})
	\item Métricas globais (arrival\_rate, dispersion\_rate, median\_duration)
	\item Métricas de qualidade (fitness, precision, simplicity)
	\item Mapeamento de recursos (opcional)
	\item Perfil do log original (\texttt{LogProfile})
\end{itemize}

\section{Etapa 3: Simulação de Logs Sintéticos}

\subsection{Diagrama de Componente}

A Figura~\ref{fig:fase3_componente} apresenta o diagrama de
componente da Etapa 3, ilustrando o processo de simulação de logs
sintéticos baseado em eventos discretos.

O componente central \texttt{LogSimulator} recebe como entradas o
objeto \texttt{ProcessModel} contendo a Rede de Petri e todos os
parâmetros estatísticos extraídos na Etapa 2, e um objeto
\texttt{SimulationConfig} que especifica os parâmetros da simulação:
\texttt{num\_cases} (padrão 100) para número de casos a serem
gerados, \texttt{arrival\_rate} (extraído do modelo) para taxa de
chegada de novos casos, \texttt{activity\_durations} (extraídos do
modelo) contendo as distribuições estatísticas de duração de cada
atividade, e \texttt{random\_seed} (padrão 42) para garantir
reprodutibilidade dos experimentos.

O processamento interno implementa um simulador de eventos discretos
utilizando a biblioteca SimPy, organizado em quatro componentes
principais: (1) gerador de chegadas, que cria novos casos seguindo
uma distribuição exponencial baseada na \texttt{arrival\_rate}
observada; (2) executor de casos, que para cada caso individual
percorre a Rede de Petri respeitando sua semântica formal,
identificando transições habilitadas e escolhendo uma aleatoriamente
para disparo; (3) gerador de eventos, que para cada transição
disparada cria um evento no log sintético com \texttt{case\_id},
\texttt{activity}, \texttt{timestamp} (tempo atual da simulação) e
\texttt{resource} (selecionado aleatoriamente do pool de recursos
disponíveis para aquela atividade); (4) controlador de duração, que
para cada atividade amostra uma duração da distribuição estatística
ajustada (Normal, Log-Normal ou Exponencial) e avança o relógio da
simulação pelo tempo correspondente. A simulação continua até que
todos os casos alcancem a marcação final da Rede de Petri ou atinjam
o limite de segurança (\texttt{max\_trace\_length=1000}) para
proteção contra loops infinitos.

A saída produzida consiste em um objeto \texttt{SimulationResult}
contendo: caminho para o arquivo CSV intermediário com os eventos
gerados em formato tabular (\texttt{case\_id}, \texttt{activity},
\texttt{time:timestamp}, \texttt{resource}); caminho para o arquivo
XES final convertido seguindo o padrão IEEE 1849-2016, incluindo
classificador de atividade para compatibilidade com ferramentas como
ProM e Disco; estatísticas da simulação incluindo
\texttt{num\_cases\_generated}, \texttt{num\_events\_generated},
\texttt{simulation\_time\_seconds} e \texttt{generation\_timestamp}.
As bibliotecas SimPy são utilizadas para implementação da lógica de
eventos discretos (processos, timeouts, environment), enquanto PM4Py
é empregada para conversão do formato intermediário CSV para o
formato XES padronizado.

\begin{figure}[htb]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.25\textwidth, keepaspectratio]{figuras/diagrama3.png}}
	\caption{Fase 3: Simulação de Logs Sintéticos. Fonte: Autor (2025)}
	\label{fig:fase3_componente}
\end{figure}

\subsection{Objetivo}

Gerar novos casos de processo que sigam o modelo descoberto e as
distribuições estatísticas extraídas, preservando características
estruturais e temporais do processo original.

\subsection{Paradigma de Simulação}

\subsubsection{Abordagem Escolhida}

O sistema utiliza \textbf{Discrete Event Simulation (DES)}
implementada com a biblioteca SimPy.

\subsubsection{Justificativa}

\noindent DES é adequado porque:
\begin{itemize}
	\item Processos de negócio são inerentemente discretos (atividades têm
	      início e fim definidos)
	\item Eventos ocorrem em pontos específicos do tempo
	\item Estado do sistema muda apenas em eventos
	\item SimPy oferece abstrações adequadas (processos, timeouts, recursos)
\end{itemize}

\subsection{Configuração da Simulação}

A Tabela~\ref{tab:parametros} lista os parâmetros principais:

\begin{table}[htb]
	\centering
	\caption{Parâmetros de configuração da simulação}
	\label{tab:parametros}
	\begin{tabular}{llp{6cm}}
		\hline
		\textbf{Parâmetro}           & \textbf{Valor Padrão} & \textbf{Justificativa}                           \\
		\hline
		\texttt{num\_cases}          & 100                   & Suficiente para análise estatística sem overhead \\
		\texttt{arrival\_rate}       & Do modelo             & Preserva taxa de chegada original                \\
		\texttt{activity\_durations} & Do modelo             & Preserva durações originais                      \\
		\texttt{random\_seed}        & 42                    & Reprodutibilidade dos experimentos               \\
		\texttt{max\_trace\_length}  & 1000                  & Limite de segurança contra loops                 \\
		\hline
	\end{tabular}
\end{table}

Todos os parâmetros podem ser sobrescritos para simulação de cenários
alternativos (por exemplo, "E se reduzirmos durações em 50\%?").

\subsection{Geração de Casos}

\subsubsection{Processo de Chegadas}

O sistema implementa um gerador SimPy que cria casos sequencialmente:

\begin{verbatim}
Para cada caso i de 1 até num_cases:
    1. Aguardar intervalo de chegada (arrival_rate)
    2. Iniciar processo paralelo para simular caso i
\end{verbatim}

Este padrão permite múltiplos casos ativos simultaneamente,
representando carga de trabalho real com concorrência.

\subsection{Simulação Individual de Casos}

O sistema implementa algoritmo baseado na semântica formal de Redes
de Petri \cite{peterson1981petri}:

\begin{algorithm}[H]
	\caption{Simulação de um caso individual}
	\begin{algorithmic}[1]
		\State \textbf{Entrada:} Rede de Petri $(net, im, fm)$
		\State $marking \gets im$ \Comment{Marcação inicial}
		\While{$marking \neq fm$} \Comment{Até marcação final}
		\State $T_{enabled} \gets$ transições habilitadas em $marking$
		\If{$T_{enabled} = \emptyset$}
		\State \textbf{break} \Comment{Marcação final ou deadlock}
		\EndIf
		\State $t \gets$ escolhe aleatoriamente de $T_{enabled}$
		\If{$t$ não é transição silenciosa}
		\State $timestamp \gets \text{tempo\_atual\_simulação}$
		\State $recurso \gets$ escolhe aleatório de $\text{recursos}(t.\text{atividade})$
		\State Registrar evento$(caso\_id, t.atividade, timestamp, recurso)$
		\State Aguardar $\text{duração}(t.\text{atividade})$ segundos
		\EndIf
		\State $marking \gets \text{executa}(t, marking)$ \Comment{Atualizar marcação}
		\If{comprimento\_trace $> max\_trace\_length$}
		\State \textbf{break} \Comment{Limite de segurança}
		\EndIf
		\EndWhile
	\end{algorithmic}
\end{algorithm}
\subsection{Geração dos Logs de Saída}

\subsubsection{Formato Intermediário (CSV)}

\begin{verbatim}
case_id,activity,time:timestamp,resource
Case 1,Register Request,2024-10-13 10:00:00,John
Case 1,Examine,2024-10-13 10:02:30,Mary
...
\end{verbatim}

\subsubsection{Conversão para XES}

\noindent Processo automatizado:

\begin{enumerate}
	\item Leitura do CSV com Pandas
	\item Renomeação de colunas para padrão XES IEEE
	\item Formatação via \texttt{pm4py.format\_dataframe()}
	\item Conversão para estrutura de log PM4Py
	\item Exportação XES via \texttt{pm4py.write\_xes()}
	\item Inserção manual de classificador (workaround limitação PM4Py)
\end{enumerate}

O classificador XES inserido:
\begin{verbatim}
<classifier name="Activity" keys="concept:name"/>
\end{verbatim}

É necessário para compatibilidade com ferramentas como ProM e Disco.

\subsection{Reprodutibilidade e Determinismo}

Um requisito fundamental da metodologia científica é a
\textbf{reprodutibilidade}: a capacidade de outros pesquisadores
replicarem exatamente os mesmos resultados a partir das mesmas
entradas. A simulação de eventos discretos, por sua natureza
estocástica, introduz aleatoriedade através de:

\begin{itemize}
	\item Amostragem de durações de atividades de distribuições estatísticas
	\item Seleção aleatória de recursos do pool disponível
	\item Escolha de transições habilitadas em marcações com múltiplas opções
	\item Geração de tempos de chegada seguindo distribuição exponencial
\end{itemize}

Para garantir reprodutibilidade total, o sistema implementa as
seguintes estratégias:

\subsubsection{Geração de Números Aleatórios Baseada em Seed}

O sistema utiliza um \textbf{seed fixo} (valor padrão: 42) para
inicialização do gerador de números pseudoaleatórios. Isto garante que,
para o mesmo seed, a mesma sequência de números aleatórios seja gerada,
resultando em logs sintéticos idênticos:

\begin{verbatim}
config = SimulationConfig(
    num_cases=100,
    random_seed=42  # Reprodutibilidade total
)
\end{verbatim}

O seed é propagado para:
\begin{itemize}
	\item \texttt{numpy.random.seed()}: amostragem de distribuições
	\item \texttt{random.seed()}: seleção de recursos e transições
	\item SimPy environment: sequenciamento de eventos
\end{itemize}

\subsubsection{Versionamento de Dependências}

O arquivo \texttt{requirements.txt} fixa versões exatas de todas as
bibliotecas:

\begin{verbatim}
pm4py==2.2.22
simpy==4.1.1
scipy==1.13.0
pandas==2.2.2
numpy==1.26.4
\end{verbatim}

Isto previne comportamento divergente causado por atualizações de
bibliotecas que possam alterar implementações de algoritmos.

\subsubsection{Logging Completo de Configurações}

Cada execução registra automaticamente:
\begin{itemize}
	\item Parâmetros de configuração utilizados
	\item Seed do gerador de números aleatórios
	\item Versões das bibliotecas carregadas
	\item Hash SHA256 do arquivo de entrada
	\item Timestamp de execução
\end{itemize}

\subsubsection{Determinismo na Ordem de Execução}

Quando múltiplas transições estão habilitadas simultaneamente em uma
marcação, a seleção é determinística dado o mesmo seed, evitando
não-determinismo causado por ordenação de estruturas de dados.

\subsubsection{Benefícios da Reprodutibilidade}

\begin{enumerate}
	\item \textbf{Validação Científica}: Outros pesquisadores podem
	      verificar os resultados apresentados
	\item \textbf{Debugging Determinístico}: Bugs podem ser reproduzidos
	      exatamente para diagnóstico
	\item \textbf{Comparação Justa}: Experimentos com diferentes
	      parâmetros partem da mesma base aleatória
	\item \textbf{Auditoria}: Resultados podem ser rastreados para suas
	      configurações exatas
	\item \textbf{Evolução Controlada}: Mudanças no código podem ser
	      testadas contra baseline reproduzível
\end{enumerate}

Para produzir resultados diferentes (por exemplo, para análise de
sensibilidade), basta alterar o seed:

\begin{verbatim}
# Executar 10 replicações independentes
for seed in range(10):
    config = SimulationConfig(random_seed=seed)
    result = simulator.simulate(model, config)
\end{verbatim}

\subsection{Saída da Etapa}

\noindent Um objeto \texttt{SimulationResult} contendo:
\begin{itemize}
	\item Caminhos dos arquivos (CSV e XES)
	\item Número de casos e eventos gerados
	\item Tempo de execução da simulação
	\item Timestamp de geração
	\item Seed utilizado (para reprodutibilidade)
	\item Configuração completa da simulação
\end{itemize}

\section{Etapa 4: Validação de Qualidade}

\subsection{Diagrama de Componente}

A Figura~\ref{fig:fase4_componente} apresenta o diagrama de
componente da Etapa 4, detalhando o processo de validação de
qualidade dos logs sintéticos gerados.

O componente central \texttt{LogValidator} recebe como entradas dois
arquivos XES: o arquivo \texttt{original\_xes} contendo o log de
eventos real e o arquivo \texttt{simulated\_xes} contendo o log
sintético gerado na Etapa 3.

O processamento interno implementa uma validação multidimensional que
compara diferentes perspectivas dos logs: (1) \textbf{Análise
	Estrutural}, onde calcula-se a similaridade dos traces usando
distância de edição; (2) \textbf{Análise de Frequência}, que compara
as distribuições de atividades e variantes; (3) \textbf{Análise
	Temporal}, que mede a similaridade das distribuições de duração das
atividades e aplica o teste de Kolmogorov-Smirnov para validação
estatística.

A saída produzida consiste em um objeto estruturado
\texttt{ValidationResult} contendo as métricas agregadas de todas as
análises, incluindo percentual de similaridade, p-valor e
estatísticas detalhadas. A biblioteca PM4Py é utilizada para os
cálculos de alinhamento e extração de características dos logs.

\begin{figure}[htb]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.25\textwidth, keepaspectratio]{figuras/diagrama4.png}}
	\caption{Fase 4: Validação de Qualidade. Fonte: Autor (2025)}
	\label{fig:fase4_componente}
\end{figure}

\subsection{Objetivo}

Avaliar quão similares são os logs original e sintético, validando a
qualidade da geração e a fidelidade do modelo. A validação é
multidimensional, comparando os logs sob perspectivas estrutural, de
frequência e temporal.

\subsection{Métricas de Validação}

O sistema calcula um conjunto de métricas para fornecer uma visão
holística da similaridade entre os logs.

\subsubsection{Similaridade Estrutural (Fitness de Alinhamento)}

A similaridade estrutural é calculada usando alinhamentos baseados em
\textbf{Distância de Edição} (\textit{Edit Distance}) entre os
traces. Para cada par de traces (um original, um sintético),
calcula-se o número mínimo de operações (inserção, deleção,
substituição) para transformar um no outro. A partir do custo do
alinhamento, deriva-se uma métrica de \textit{fitness} normalizada:

\begin{equation}
	fitness = 1 - \frac{\text{custo\_alinhamento}}{\text{custo\_máximo}}
\end{equation}

Onde $\text{custo\_máximo}$ é o custo do pior caso (deletar todos os
eventos de um trace e inserir todos do outro). O \textit{fitness}
médio de todos os alinhamentos resulta em um percentual de
similaridade estrutural.

\subsubsection{Similaridade de Distribuição de Atividades}

Esta métrica compara a frequência relativa de cada atividade entre o
log original e o sintético. Utiliza-se a distância de Jaccard ou
similar para quantificar a sobreposição das distribuições de
frequência. Um valor próximo de 1.0 indica que as atividades mais
comuns no processo real também são as mais comuns no processo
simulado.

\subsubsection{Similaridade de Distribuição de Variantes}

Compara a distribuição das variantes (sequências de atividades
únicas) entre os dois logs. Esta métrica avalia se o modelo simulado
consegue reproduzir os caminhos mais frequentes do processo real com
uma frequência parecida. Assim como na métrica de atividades, um
valor próximo de 1.0 é desejável.

\subsubsection{Similaridade de Distribuição de Durações}

Avalia se as durações das atividades no log sintético seguem uma
distribuição estatística similar às do log original. A comparação é
feita utilizando a distância de Wasserstein (Earth Mover's Distance)
ou outra métrica de distância entre distribuições, que mede o
"esforço" necessário para transformar uma distribuição na outra.

\subsubsection{Validação Estatística (Teste de Kolmogorov-Smirnov)}

Para uma validação mais rigorosa das durações, o sistema aplica o
teste de Kolmogorov-Smirnov (K-S) de duas amostras. Este teste não
paramétrico avalia a hipótese nula de que as amostras de duração
(original e sintética) foram extraídas da mesma distribuição. O
resultado é um \textbf{p-valor}:

\begin{itemize}
	\item \textbf{p-valor > 0.05}: Não há evidência estatística para rejeitar a hipótese nula. As distribuições são consideradas estatisticamente similares.
	\item \textbf{p-valor $\leq$ 0.05}: A hipótese nula é rejeitada. As distribuições são consideradas estatisticamente diferentes.
\end{itemize}

Um p-valor alto indica que a simulação reproduz fielmente a
variabilidade temporal do processo original.

\subsection{Agregação e Interpretação de Resultados}

As métricas são agregadas em um placar de validação. A
Tabela~\ref{tab:interpretacao} apresenta os thresholds para
interpretação.

\begin{table}[htb]
	\centering
	\caption{Interpretação de métricas de validação}
	\label{tab:interpretacao}
	\begin{tabular}{ll}
		\hline
		\textbf{Métrica}          & \textbf{Interpretação}                     \\
		\hline
		Similaridade Estrutural   & $\geq 80\%$ (Boa), $\geq 90\%$ (Excelente) \\
		Similaridade (Atividades) & $\geq 0.8$ (Boa)                           \\
		Similaridade (Variantes)  & $\geq 0.7$ (Boa)                           \\
		Similaridade (Durações)   & $\geq 0.8$ (Boa)                           \\
		KS-Test p-valor           & $> 0.05$ (Similaridade Estatística)        \\
		\hline
	\end{tabular}
\end{table}

\subsection{Saída da Etapa}

Um objeto \texttt{ValidationResult} contendo:
\begin{itemize}
	\item Percentual de similaridade estrutural (baseado em fitness médio)
	\item Similaridade de distribuição de atividades, variantes e durações
	\item P-valor do teste de Kolmogorov-Smirnov
	\item Detalhes estatísticos (médias, mínimos, máximos de fitness e custo)
\end{itemize}

\section{Componente Independente: Cálculo de Métricas ORE}

\subsection{Posicionamento no Sistema}

O \textbf{ORECalculator} é um componente independente e opcional do
sistema, especializado em calcular métricas de eficiência operacional
para processos hospitalares, especificamente para salas cirúrgicas. Ao
contrário dos quatro componentes anteriores que formam o pipeline
sequencial obrigatório (LogAnalyzer → ProcessMiner → LogSimulator →
LogValidator), o ORECalculator opera de forma \textbf{paralela e
	independente}, podendo ser invocado diretamente sobre o log original
sem necessidade de executar as etapas de mineração ou simulação.

\subsection{Arquitetura do Componente}

\begin{figure}[htb]
	\centering
	\fbox{\begin{minipage}{0.9\textwidth}
			\small
			\begin{verbatim}
┌─────────────────────────────────────────────────────┐
│          Componente ORECalculator                   │
├─────────────────────────────────────────────────────┤
│ Entrada:                                            │
│  - log.xes (arquivo XES)                            │
│  - daily_hours: float (padrão: 11.5h)               │
│  - setup_time_minutes: float (padrão: 15 min)       │
├─────────────────────────────────────────────────────┤
│ Processamento:                                      │
│  1. Detecção de atributos enriquecidos             │
│     - surgery:status                                │
│     - surgery:actual_duration                       │
│     - surgery:cancellation_reason                   │
│  2. Extração de componentes temporais              │
│     - TTA (Total Time Available)                    │
│     - TTS (Total Time Scheduled)                    │
│     - TTU (Total Time Used)                         │
│     - TTAV (Total Time Added Value)                 │
│  3. Cálculo de indicadores ORE                     │
│     - A (Availability) = TTS / TTA                  │
│     - P (Performance) = TTU / TTS                   │
│     - Q (Quality) = TTAV / TTU                      │
│     - ORE = A × P × Q                               │
│  4. Decomposição de perdas (7 categorias)          │
│  5. Análise de cancelamentos                       │
├─────────────────────────────────────────────────────┤
│ Saída:                                              │
│  - OREMetrics (dataclass)                           │
│    - ore: float                                     │
│    - availability, performance, quality: float      │
│    - loss_breakdown: Dict[str, float]               │
│    - cancellation_metrics: Dict                     │
└─────────────────────────────────────────────────────┘
\end{verbatim}
		\end{minipage}}
	\caption{Arquitetura do componente ORECalculator. Fonte: Autor (2025)}
	\label{fig:ore_component}
\end{figure}

\subsection{Justificativa da Separação Arquitetural}

A decisão de implementar o ORECalculator como componente independente
fundamenta-se em três razões principais:

\begin{enumerate}
	\item \textbf{Especificidade de Domínio}: As métricas ORE são
	      específicas para processos hospitalares (salas cirúrgicas),
	      enquanto os componentes do pipeline principal são
	      \textbf{domain-agnostic}, funcionando para qualquer tipo de
	      processo de negócio (manufatura, finanças, logística, etc.).

	\item \textbf{Independência Funcional}: O cálculo de ORE não depende
	      da execução do pipeline de simulação. Um analista hospitalar
	      pode querer apenas calcular indicadores de eficiência do log
	      real, sem interesse em gerar logs sintéticos.

	\item \textbf{Extensibilidade}: Esta arquitetura permite adicionar
	      outros calculadores específicos de domínio no futuro (por
	      exemplo, métricas financeiras, KPIs de manufatura) sem acoplar
	      ao pipeline principal.
\end{enumerate}

\subsection{Framework ORE (Operating Room Effectiveness)}

O sistema implementa o framework ORE proposto por Souza et al.
\cite{souza2020ore}, que decompõe a efetividade operacional de salas
cirúrgicas em três dimensões multiplicativas:

\begin{equation}
	\text{ORE} = \text{Disponibilidade} \times \text{Desempenho} \times \text{Qualidade}
\end{equation}

\subsubsection{Disponibilidade (A)}

Representa a razão entre o tempo total programado e o tempo total
disponível:

\begin{equation}
	A = \frac{\text{TTS}}{\text{TTA}}
\end{equation}

\textbf{Interpretação}: Que percentual do tempo disponível foi
efetivamente agendado para cirurgias?

\textbf{Categorias de perdas que impactam A}:
\begin{itemize}
	\item Falha de equipamento
	\item Tempo de setup (preparação da sala)
	\item Não agendamento (slots vagos)
\end{itemize}

\subsubsection{Desempenho (P)}

Representa a razão entre o tempo total utilizado e o tempo total
programado:

\begin{equation}
	P = \frac{\text{TTU}}{\text{TTS}}
\end{equation}

\textbf{Interpretação}: Que percentual do tempo agendado foi
efetivamente utilizado?

\textbf{Categorias de perdas que impactam P}:
\begin{itemize}
	\item Pequenas paradas (atrasos operacionais)
	\item Variação no tempo cirúrgico
	\item Cancelamentos
\end{itemize}

\subsubsection{Qualidade (Q)}

Representa a razão entre o tempo total de valor agregado e o tempo
total utilizado:

\begin{equation}
	Q = \frac{\text{TTAV}}{\text{TTU}}
\end{equation}

\textbf{Interpretação}: Que percentual do tempo utilizado agregou valor
real (sem retrabalho)?

\textbf{Categorias de perdas que impactam Q}:
\begin{itemize}
	\item Reintervenções (cirurgias repetidas)
\end{itemize}

\subsection{Suporte a XES Enriquecido}

O componente possui capacidade de processar dois tipos de logs XES:

\subsubsection{XES Padrão}

Quando o log contém apenas atributos padrão (activity, timestamp,
resource), o sistema \textbf{estima} as métricas ORE baseando-se em
heurísticas:

\begin{itemize}
	\item Duração planejada estimada pela mediana das durações observadas
	\item Cancelamentos inferidos de casos incompletos
	\item Perdas estimadas por valores padrão da literatura
\end{itemize}

\subsubsection{XES Enriquecido}

Quando o log contém atributos customizados específicos do domínio
hospitalar, o sistema calcula métricas \textbf{exatas}:

\textbf{Atributos enriquecidos suportados}:
\begin{itemize}
	\item \texttt{surgery:status}: status da cirurgia (Realizada,
	      Cancelada, Pendente)
	\item \texttt{surgery:actual\_duration}: duração real da cirurgia em
	      minutos
	\item \texttt{surgery:cancellation\_reason}: motivo do cancelamento
	      (Paciente, Equipamento, Recurso)
	\item \texttt{surgery:reintervention}: flag booleana indicando
	      reintervenção
\end{itemize}

\textbf{Exemplo de trace enriquecido}:
\begin{verbatim}
<trace>
  <string key="concept:name" value="Case001"/>
  <event>
    <string key="concept:name" value="Surgery"/>
    <date key="time:timestamp" value="2024-10-13T08:00:00"/>
    <string key="surgery:status" value="Realizada"/>
    <int key="surgery:actual_duration" value="125"/>
    <boolean key="surgery:reintervention" value="false"/>
  </event>
</trace>
\end{verbatim}

\subsection{Decomposição Granular de Perdas}

O sistema categoriza perdas operacionais em sete categorias conforme o
framework ORE:

\begin{table}[htb]
	\centering
	\caption{Categorias de perdas operacionais no framework ORE}
	\label{tab:ore_losses}
	\small
	\begin{tabular}{p{4cm}p{8cm}}
		\hline
		\textbf{Categoria de Perda}        & \textbf{Descrição}                                                 \\
		\hline
		Falha de Equipamento               & Tempo perdido por quebra ou indisponibilidade de equipamentos      \\
		Setup                              & Tempo de preparação da sala entre cirurgias (padrão: 15 min/caso) \\
		Não Agendamento                    & Slots disponíveis não utilizados (TTA - TTS)                       \\
		Pequenas Paradas                   & Interrupções breves durante procedimentos                          \\
		Variação no Tempo Cirúrgico        & Diferença entre tempo planejado e tempo real                       \\
		Cancelamentos                      & Cirurgias agendadas mas não realizadas                             \\
		Reintervenções                     & Repetição de procedimentos (retrabalho)                            \\
		\hline
	\end{tabular}
\end{table}

\subsection{Integração com o Pipeline}

Embora independente, o ORECalculator pode ser invocado através da
fachada \texttt{ProcessMiningPipeline}:

\begin{verbatim}
# Uso independente
ore_calc = ORECalculator(daily_hours=11.5, setup_time_minutes=15)
metrics = ore_calc.calculate_from_log("surgical_log.xes")

# Uso através do pipeline
pipeline = ProcessMiningPipeline()
ore_metrics = pipeline.calculate_ore("surgical_log.xes")

# Uso integrado com pipeline completo
results = pipeline.run_full_pipeline("surgical_log.xes")
# results['ore_metrics'] contém OREMetrics (se aplicável)
\end{verbatim}

\subsection{Saída do Componente}

Um objeto \texttt{OREMetrics} contendo:
\begin{itemize}
	\item \textbf{Indicadores}: ore, availability, performance, quality (0-1)
	\item \textbf{Componentes temporais}: TTA, TTS, TTU, TTAV (horas)
	\item \textbf{Decomposição de perdas}: Dict com 7 categorias (horas)
	\item \textbf{Estatísticas de cancelamento}: taxa, motivos, contagens
	\item \textbf{Cobertura de dados}: percentual de eventos com cada
	      atributo enriquecido
\end{itemize}

\section{Interfaces de Acesso ao Sistema}

O Sim2Log-Core oferece duas interfaces complementares para diferentes
perfis de usuários, implementando uma estratégia de
\textbf{multi-interface} que amplia a acessibilidade do sistema.

\subsection{Interface Programática (API Python)}

\subsubsection{Público-Alvo}

Pesquisadores, cientistas de dados e desenvolvedores que necessitam
integrar o sistema em scripts, notebooks Jupyter ou pipelines de
análise automatizados.

\subsubsection{Características}

\textbf{Alto nível (via Facade)}:
\begin{verbatim}
from pipeline import ProcessMiningPipeline, quick_analysis

# Uso simplificado
results = quick_analysis("log.xes")

# Uso com controle
pipeline = ProcessMiningPipeline(verbose=True)
profile = pipeline.analyze_log("log.xes")
model = pipeline.mine_process("log.xes", variant_filter=0.8)
sim_result = pipeline.simulate(model, num_cases=100)
validation = pipeline.validate("original.xes", "synthetic.xes")
\end{verbatim}

\textbf{Nível de componente (uso independente)}:
\begin{verbatim}
from log_analyzer import LogAnalyzer
from process_mining import ProcessMiner
from simulation import LogSimulator, SimulationConfig

# Uso granular de componentes individuais
analyzer = LogAnalyzer()
profile = analyzer.analyze("log.xes")

miner = ProcessMiner()
model = miner.mine_process("log.xes")

config = SimulationConfig(num_cases=200, random_seed=123)
simulator = LogSimulator(config)
result = simulator.simulate(model)
\end{verbatim}

\subsubsection{Vantagens}

\begin{itemize}
	\item \textbf{Automação}: Processamento em lote de múltiplos logs
	\item \textbf{Integração}: Incorporação em workflows existentes
	\item \textbf{Flexibilidade}: Acesso granular a componentes individuais
	\item \textbf{Reprodutibilidade}: Scripts versionados garantem
	      replicação exata
	\item \textbf{Programabilidade}: Controle total via código Python
\end{itemize}

\subsection{Interface Web Interativa (Streamlit Dashboard)}

\subsubsection{Público-Alvo}

Analistas de negócios, profissionais de saúde, gestores hospitalares e
usuários sem conhecimento de programação que necessitam de análise
exploratória e validação de processos.

\subsubsection{Arquitetura da Interface}

\begin{figure}[htb]
	\centering
	\fbox{\begin{minipage}{0.9\textwidth}
			\small
			\begin{verbatim}
┌─────────────────────────────────────────────────────┐
│           Streamlit Web Dashboard                   │
│                 (app/app.py)                        │
├─────────────────────────────────────────────────────┤
│                                                     │
│  [Upload XES File]                                  │
│       │                                             │
│       ▼                                             │
│  ┌───────────────────────────────────────────┐     │
│  │  Modo de Execução                         │     │
│  │  ○ Executar Tudo (pipeline completo)      │     │
│  │  ○ Passo-a-Passo (controle granular)      │     │
│  └───────────────────────────────────────────┘     │
│                                                     │
│  ┌─ Etapa 1: Análise ─────────────────────┐        │
│  │ ✓ Detectados 1500 casos                │        │
│  │ ✓ Detectados 25 atividades             │        │
│  │ [Ver Estatísticas Detalhadas]          │        │
│  └────────────────────────────────────────┘        │
│                                                     │
│  ┌─ Etapa 2: Mineração ────────────────────┐       │
│  │ ✓ Modelo descoberto (Inductive Miner)  │        │
│  │ ✓ Fitness: 0.92 | Precision: 0.88      │        │
│  │ [Visualizar Rede de Petri]             │        │
│  │ [Download: model.pnml]                 │        │
│  └────────────────────────────────────────┘        │
│                                                     │
│  ┌─ Etapa 3: Simulação ────────────────────┐       │
│  │ Número de casos: [100] ▼                │       │
│  │ Seed: [42]                              │       │
│  │ ✓ Gerados 100 casos sintéticos         │        │
│  │ [Download: synthetic.xes]               │       │
│  │ [Download: synthetic.csv]               │       │
│  └────────────────────────────────────────┘        │
│                                                     │
│  ┌─ Etapa 4: Validação ────────────────────┐       │
│  │ ✓ Similaridade: 91.2%                  │        │
│  │ ✓ Fitness: 0.94                        │        │
│  │ ✓ KS-test p-value: 0.23                │        │
│  │ [Ver Comparação Detalhada]             │        │
│  └────────────────────────────────────────┘        │
│                                                     │
│  ┌─ ORE (Opcional) ─────────────────────────┐      │
│  │ ✓ ORE: 68.7%                            │       │
│  │ ✓ Disponibilidade: 82% | Desempenho: 91%│       │
│  │ [Ver Decomposição de Perdas]            │       │
│  └────────────────────────────────────────┘        │
│                                                     │
└─────────────────────────────────────────────────────┘
\end{verbatim}
		\end{minipage}}
	\caption{Arquitetura da interface web Streamlit. Fonte: Autor (2025)}
	\label{fig:web_interface}
\end{figure}

\subsubsection{Funcionalidades}

\begin{enumerate}
	\item \textbf{Upload de Arquivos}: Drag-and-drop de arquivos XES
	\item \textbf{Execução Dual-Mode}:
	      \begin{itemize}
		      \item \textit{Run Everything}: Pipeline completo com um clique
		      \item \textit{Step-by-Step}: Controle individual de cada etapa
	      \end{itemize}
	\item \textbf{Visualizações Interativas}:
	      \begin{itemize}
		      \item Diagrama da Rede de Petri (via Graphviz)
		      \item Gráficos de barras de frequência de atividades
		      \item Distribuição de variantes do processo
		      \item Comparação visual de métricas de qualidade
	      \end{itemize}
	\item \textbf{Download de Resultados}:
	      \begin{itemize}
		      \item Logs sintéticos (XES e CSV)
		      \item Modelo de processo (PNML)
		      \item Relatórios de validação (JSON)
		      \item Métricas ORE (JSON)
	      \end{itemize}
	\item \textbf{Feedback em Tempo Real}: Barras de progresso e mensagens
	      de status
	\item \textbf{Tratamento de Erros}: Mensagens descritivas para
	      problemas comuns
\end{enumerate}

\subsubsection{Implementação Técnica}

A interface é implementada usando Streamlit 1.28.0, que permite criar
aplicações web interativas usando apenas Python, sem necessidade de
conhecimento de HTML, CSS ou JavaScript. A comunicação com o backend
ocorre através da instância \texttt{ProcessMiningPipeline}, mantendo a
separação entre interface e lógica de negócio.

\textbf{Execução}:
\begin{verbatim}
streamlit run app/app.py
# Acesso em http://localhost:8501
\end{verbatim}

\subsubsection{Vantagens}

\begin{itemize}
	\item \textbf{Acessibilidade}: Usuários não-técnicos podem utilizar o sistema
	\item \textbf{Exploração Interativa}: Ajuste de parâmetros em tempo real
	\item \textbf{Visualização Imediata}: Resultados apresentados graficamente
	\item \textbf{Sem Instalação Cliente}: Acesso via navegador web
	\item \textbf{Compartilhamento Fácil}: Interface pode ser hospedada e compartilhada
\end{itemize}

\subsection{Comparação entre Interfaces}

\begin{table}[htb]
	\centering
	\caption{Comparação entre as interfaces de acesso}
	\label{tab:interface_comparison}
	\small
	\begin{tabular}{p{3cm}p{5cm}p{5cm}}
		\hline
		\textbf{Critério}     & \textbf{API Programática}               & \textbf{Interface Web}                    \\
		\hline
		Público-alvo          & Pesquisadores, desenvolvedores          & Analistas, gestores                       \\
		Conhecimento requer.  & Python intermediário                    & Nenhum (uso intuitivo)                    \\
		Controle              & Total (nível de código)                 & Limitado a parâmetros expostos            \\
		Automação             & Fácil (scripts, batch)                  & Difícil (interativa)                      \\
		Visualização          & Requer código adicional                 & Integrada                                 \\
		Reprodutibilidade     & Excelente (scripts versionados)         & Moderada (config manual)                  \\
		Curva de aprendizado  & Íngreme                                 & Suave                                     \\
		Uso típico            & Análise em lote, integração, pesquisa   & Exploração, validação, demonstração       \\
		\hline
	\end{tabular}
\end{table}

\section{Parâmetros e Configurações}

\subsection{Tabela de Parâmetros Principais}

A Tabela~\ref{tab:parametros_completos} apresenta todos os parâmetros
configuráveis do sistema:

\begin{table}[htb]
	\centering
	\caption{Parâmetros configuráveis do sistema}
	\label{tab:parametros_completos}
	\begin{tabular}{lccl}
		\hline
		\textbf{Parâmetro}          & \textbf{Padrão} & \textbf{Faixa}   & \textbf{Impacto}                \\
		\hline
		\texttt{variant\_filter}    & 0.8             & 0.0-1.0          & Filtragem de ruído na mineração \\
		\texttt{num\_cases}         & 100             & 1-$\infty$       & Tamanho do log sintético        \\
		\texttt{arrival\_rate}      & Auto            & 0.1-$\infty$ min & Taxa de chegada de casos        \\
		\texttt{random\_seed}       & 42              & 0-$2^{32}$-1     & Reprodutibilidade               \\
		\texttt{max\_trace\_length} & 1000            & 1-$\infty$       & Proteção contra loops           \\
		\texttt{verbose}            & True            & True/False       & Saída de diagnóstico            \\
		\texttt{save\_model\_image} & None            & Path/None        & Visualização do modelo          \\
		\hline
	\end{tabular}
\end{table}

\section{Ferramentas e Tecnologias}

\subsection{Bibliotecas Principais}

A Tabela~\ref{tab:bibliotecas} apresenta as tecnologias utilizadas.

\begin{table}[htb]
	\centering
	\caption{Bibliotecas e ferramentas utilizadas}
	\label{tab:bibliotecas}
	\small
	\begin{tabular}{llp{6cm}}
		\hline
		\textbf{Biblioteca} & \textbf{Versão} & \textbf{Justificativa}                                                     \\
		\hline
		PM4Py               & 2.2.22          & Padrão de facto em Python para process mining, algoritmos state-of-the-art \\
		SimPy               & 4.0.1           & Leve, Pythônico, documentação excelente para DES                           \\
		SciPy               & 1.13.0          & Completa, bem testada, K-S test built-in                                   \\
		Pandas              & 2.2.2           & Eficiente para manipulação de dados, operações vetorizadas                 \\
		NumPy               & 1.26.4          & Base do ecossistema científico Python                                      \\
		Streamlit           & 1.28.0          & Interface web rápida e intuitiva                                           \\
		Graphviz            & 0.20.3          & Visualização de Redes de Petri                                             \\
		\hline
	\end{tabular}
\end{table}

\subsection{Justificativa das Escolhas}

A escolha das bibliotecas foi fundamentada em critérios técnicos
específicos para cada funcionalidade do sistema. O PM4Py representa o
padrão de facto em Python para process mining, oferecendo algoritmos
state-of-the-art com documentação completa. O SimPy foi selecionado
para simulação discreta de eventos por sua simplicidade e adequação
perfeita para modelagem de sistemas discretos. O SciPy foi escolhido
para análise estatística por sua robustez e confiabilidade.

Para manipulação de dados, o Pandas foi selecionado por sua
eficiência em operações vetorizadas, essencial para processamento dos
logs de eventos. O NumPy serve como base fundamental do ecossistema
científico Python, proporcionando operações matemáticas otimizadas. O
Streamlit foi escolhido para criar interfaces web rápidas e
intuitivas, facilitando a interação com o sistema. O Graphviz foi
selecionado especificamente para visualização de Redes de Petri,
oferecendo capacidades gráficas adequadas para representação dos
modelos de processo descobertos.

\section{Pseudocódigo de Alto Nível}

O algoritmo~\ref{alg:pipeline} apresenta o pipeline completo:

\begin{algorithm}[H]
	\caption{Pipeline completo de geração de logs sintéticos}
	\label{alg:pipeline}
	\begin{algorithmic}[1]
		\Function{GenerateSyntheticLog}{$original\_xes\_path$}
		\State \textbf{// Etapa 1: Análise}
		\State $analyzer \gets$ \Call{LogAnalyzer}{}
		\State $profile \gets analyzer.analyze(original\_xes\_path)$
		\State
		\State \textbf{// Etapa 2: Mineração}
		\State $miner \gets$ \Call{ProcessMiner}{verbose=True}
		\State $model \gets miner.mine\_process(original\_xes\_path, variant\_filter=0.8)$
		\State
		\State \textbf{// Etapa 3: Simulação}
		\State $config \gets$ \Call{SimulationConfig}{num\_cases=100, random\_seed=42}
		\State $simulator \gets$ \Call{LogSimulator}{$config$, verbose=True}
		\State $result \gets simulator.simulate(model, output\_dir)$
		\State
		\State \textbf{// Etapa 4: Validação}
		\State $validator \gets$ \Call{LogValidator}{verbose=True}
		\State $validation \gets validator.validate(original\_xes\_path, result.xes\_path)$
		\State
		\State \Return $(result, validation)$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

% ----------------------------------------------------------
% Desenvolvimento
% ----------------------------------------------------------
